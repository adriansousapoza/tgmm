

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Metrics &mdash; TorchGMM 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=01f34227"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Priors" href="priors.html" />
    <link rel="prev" title="Tutorial: A Gaussian Mixture Model" href="gmm.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            TorchGMM
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../source/modules.html">Modules</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../source/tutorials.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="gmm.html">Tutorial: A Gaussian Mixture Model</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="priors.html">Priors</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">TorchGMM</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../source/tutorials.html">Tutorials</a></li>
      <li class="breadcrumb-item active">Metrics</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/notebooks/metrics.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Metrics">
<h1>Metrics<a class="headerlink" href="#Metrics" title="Link to this heading">ÔÉÅ</a></h1>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import torch
from tqdm import tqdm
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
from matplotlib.colors import ListedColormap
import seaborn as sns
import pandas as pd
import importlib
import os
import sys
sys.path.append(&#39;../../..&#39;)
#print(os.listdir(&quot;../../..&quot;))

from utils.metrics import ClusteringMetrics
from utils.gmm import GaussianMixture

# Scikit-learn for comparison
from sklearn.mixture import GaussianMixture as SklearnGMM
from sklearn.decomposition import PCA
from sklearn.metrics import (
    silhouette_score,
    davies_bouldin_score,
    calinski_harabasz_score,
    adjusted_rand_score,
    normalized_mutual_info_score,
    fowlkes_mallows_score,
    homogeneity_score,
    mutual_info_score,
    adjusted_mutual_info_score,
    completeness_score,
    v_measure_score,
    rand_score
)

# Detect device
device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
print(&quot;Using device:&quot;, device)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Using device: cuda
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>np.random.seed(0)  # for reproducibility
n_samples_1 = 1000
n_samples_2 = 800
n_samples_3 = 400
n_samples_4 = 600

center_1 = np.array([0, 0, 0, 0])
center_2 = np.array([-4, 4, -4, 4])
center_3 = np.array([4, -4, 4, -4])
center_4 = np.array([4, 4, 4, 4])

# Random transformation matrices to generate covariances
C_1 = np.random.rand(4, 4)
C_2 = np.random.rand(4, 4)

component_1 = np.dot(np.random.randn(n_samples_1, 4), C_1) + center_1
component_2 = 0.7 * np.random.randn(n_samples_2, 4) + center_2
component_3 = 0.5 * np.random.randn(n_samples_3, 4) + center_3
component_4 = np.dot(np.random.randn(n_samples_4, 4), C_2) + center_4

X = np.concatenate([component_1, component_2, component_3, component_4], axis=0)
y_true = np.concatenate([
    np.zeros(n_samples_1),
    np.ones(n_samples_2),
    np.full(n_samples_3, 2),
    np.full(n_samples_4, 3)
])

X_tensor = torch.tensor(X, dtype=torch.float32, device=device)
y_tensor = torch.tensor(y_true, dtype=torch.long, device=device)

true_n_components = 4
print(f&quot;Data shape: {X_tensor.shape}, true number of clusters: {true_n_components}&quot;)
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Data shape: torch.Size([2800, 4]), true number of clusters: 4
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>components_range = np.arange(2, 11)
silhouette_vals = torch.zeros(len(components_range), device=device)
davies_vals = torch.zeros(len(components_range), device=device)
calinski_vals = torch.zeros(len(components_range), device=device)
dunn_vals = torch.zeros(len(components_range), device=device)
bic_vals = torch.zeros(len(components_range), device=device)
aic_vals = torch.zeros(len(components_range), device=device)

# Fit a GMM for each n in components_range
for i, n in tqdm(enumerate(components_range), total=len(components_range), desc=&quot;Evaluating range&quot;):
    gmm = GaussianMixture(
        n_features=4,
        n_components=n,
        covariance_type=&#39;full&#39;,
        max_iter=1000,
        init_params=&#39;kmeans&#39;,
        device=device
    )
    # Fit
    gmm.fit(X_tensor)
    labels_pred = gmm.predict(X_tensor)  # shape (N,)

    # Compute unsupervised metrics
    silhouette_vals[i] = ClusteringMetrics.silhouette_score(X_tensor, labels_pred, n_components=n)
    davies_vals[i] = ClusteringMetrics.davies_bouldin_index(X_tensor, labels_pred, n_components=n)
    calinski_vals[i] = ClusteringMetrics.calinski_harabasz_score(X_tensor, labels_pred, n_components=n)
    dunn_vals[i] = ClusteringMetrics.dunn_index(X_tensor, labels_pred, n_components=n)
    bic_vals[i] = ClusteringMetrics.bic_score(gmm.lower_bound_, X_tensor, n, gmm.covariance_type)
    aic_vals[i] = ClusteringMetrics.aic_score(gmm.lower_bound_, X_tensor, n, gmm.covariance_type)

fig, axs = plt.subplots(2, 3, figsize=(16, 8))

# 1) Silhouette
axs[0, 0].plot(components_range, silhouette_vals.cpu(), &#39;o-b&#39;)
axs[0, 0].axvline(x=true_n_components, color=&#39;r&#39;, linestyle=&#39;--&#39;, label=&#39;True # of Components&#39;)
axs[0, 0].set_title(&#39;Silhouette Score&#39;)
axs[0, 0].set_xlabel(&#39;Number of Components&#39;)
axs[0, 0].set_ylabel(&#39;Score&#39;)
axs[0, 0].grid(True)
axs[0, 0].legend()

# 2) Davies-Bouldin
axs[0, 1].plot(components_range, davies_vals.cpu(), &#39;o-g&#39;)
axs[0, 1].axvline(x=true_n_components, color=&#39;r&#39;, linestyle=&#39;--&#39;)
axs[0, 1].set_title(&#39;Davies-Bouldin Index&#39;)
axs[0, 1].set_xlabel(&#39;Number of Components&#39;)
axs[0, 1].grid(True)

# 3) Calinski-Harabasz
axs[0, 2].plot(components_range, calinski_vals.cpu(), &#39;o-y&#39;)
axs[0, 2].axvline(x=true_n_components, color=&#39;r&#39;, linestyle=&#39;--&#39;)
axs[0, 2].set_title(&#39;Calinski-Harabasz Score&#39;)
axs[0, 2].set_xlabel(&#39;Number of Components&#39;)
axs[0, 2].grid(True)

# 4) Dunn Index
axs[1, 0].plot(components_range, dunn_vals.cpu(), &#39;o-&#39;, color=&#39;orange&#39;)
axs[1, 0].axvline(x=true_n_components, color=&#39;r&#39;, linestyle=&#39;--&#39;)
axs[1, 0].set_title(&#39;Dunn Index&#39;)
axs[1, 0].set_xlabel(&#39;Number of Components&#39;)
axs[1, 0].grid(True)

# 5) BIC
axs[1, 1].plot(components_range, bic_vals.cpu(), &#39;o-m&#39;, label=&#39;BIC&#39;)
axs[1, 1].axvline(x=true_n_components, color=&#39;r&#39;, linestyle=&#39;--&#39;)
axs[1, 1].set_title(&#39;BIC Score&#39;)
axs[1, 1].set_xlabel(&#39;Number of Components&#39;)
axs[1, 1].grid(True)
axs[1, 1].legend()

# 6) AIC
axs[1, 2].plot(components_range, aic_vals.cpu(), &#39;o-c&#39;, label=&#39;AIC&#39;)
axs[1, 2].axvline(x=true_n_components, color=&#39;r&#39;, linestyle=&#39;--&#39;)
axs[1, 2].set_title(&#39;AIC Score&#39;)
axs[1, 2].set_xlabel(&#39;Number of Components&#39;)
axs[1, 2].grid(True)
axs[1, 2].legend()

plt.suptitle(&quot;Unsupervised Clustering Metrics for GMM (4D Data)&quot;, fontsize=16)
plt.tight_layout()
plt.show()

print(&quot;=== Best Number of Components According to Each Metric ===&quot;)
print(f&quot;Silhouette Best: {components_range[torch.argmax(silhouette_vals)].item()}&quot;)
print(f&quot;Davies-Bouldin Best (lowest): {components_range[torch.argmin(davies_vals)].item()}&quot;)
print(f&quot;Calinski-Harabasz Best: {components_range[torch.argmax(calinski_vals)].item()}&quot;)
print(f&quot;Dunn Index Best: {components_range[torch.argmax(dunn_vals)].item()}&quot;)
print(f&quot;BIC Best (lowest): {components_range[torch.argmin(bic_vals)].item()}&quot;)
print(f&quot;AIC Best (lowest): {components_range[torch.argmin(aic_vals)].item()}&quot;)
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Evaluating range: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00&lt;00:00, 17.32it/s]
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_metrics_3_1.png" src="../_images/notebooks_metrics_3_1.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
=== Best Number of Components According to Each Metric ===
Silhouette Best: 9
Davies-Bouldin Best (lowest): 4
Calinski-Harabasz Best: 4
Dunn Index Best: 4
BIC Best (lowest): 4
AIC Best (lowest): 5
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n_components = true_n_components  # 4
gmm = GaussianMixture(
    n_features=4,
    n_components=n_components,
    covariance_type=&#39;full&#39;,
    max_iter=1000,
    init_params=&#39;kmeans&#39;,
    device=device
)
gmm.fit(X_tensor)
labels_pred = gmm.predict(X_tensor)

# Metrics with custom ClusteringMetrics
requested_metrics = [
    &quot;rand_score&quot;,
    &quot;adjusted_rand_score&quot;,
    &quot;mutual_info_score&quot;,
    &quot;normalized_mutual_info_score&quot;,
    &quot;adjusted_mutual_info_score&quot;,
    &quot;fowlkes_mallows_score&quot;,
    &quot;homogeneity_score&quot;,
    &quot;completeness_score&quot;,
    &quot;v_measure_score&quot;,
    &quot;purity_score&quot;,
    &quot;silhouette_score&quot;,
    &quot;davies_bouldin_index&quot;,
    &quot;calinski_harabasz_score&quot;,
    &quot;dunn_index&quot;,
    &quot;bic_score&quot;,
    &quot;aic_score&quot;,
]
scores_torch = ClusteringMetrics.evaluate_clustering(
    gmm,
    X_tensor,
    true_labels=y_tensor,
    metrics=requested_metrics
)
print(&quot;=== Torch Metrics for n_components=4 ===&quot;)
for k, v in scores_torch.items():
    # Some items (e.g. classification_report) could be nested or dict, so check type
    if isinstance(v, (int, float)):
        print(f&quot;{k}: {v:.4f}&quot;)
    else:
        print(f&quot;{k}: {v}&quot;)

# Compare with sklearn
X_np = X_tensor.cpu().numpy()
y_np = y_tensor.cpu().numpy()
labels_pred_np = labels_pred.cpu().numpy()

sk_gmm = SklearnGMM(n_components=n_components, covariance_type=&#39;full&#39;, max_iter=1000, init_params=&#39;kmeans&#39;)
sk_gmm.fit(X_np)

scores_sklearn = {
    &quot;rand_score&quot;: rand_score(y_np, labels_pred_np),
    &quot;adjusted_rand_score&quot;: adjusted_rand_score(y_np, labels_pred_np),
    &quot;mutual_info_score&quot;: mutual_info_score(y_np, labels_pred_np),
    &quot;normalized_mutual_info_score&quot;: normalized_mutual_info_score(y_np, labels_pred_np),
    &quot;adjusted_mutual_info_score&quot;: adjusted_mutual_info_score(y_np, labels_pred_np),
    &quot;fowlkes_mallows_score&quot;: fowlkes_mallows_score(y_np, labels_pred_np),
    &quot;homogeneity_score&quot;: homogeneity_score(y_np, labels_pred_np),
    &quot;completeness_score&quot;: completeness_score(y_np, labels_pred_np),
    &quot;v_measure_score&quot;: v_measure_score(y_np, labels_pred_np),
    &quot;silhouette_score&quot;: silhouette_score(X_np, labels_pred_np),
    &quot;davies_bouldin_index&quot;: davies_bouldin_score(X_np, labels_pred_np),
    &quot;calinski_harabasz_score&quot;: calinski_harabasz_score(X_np, labels_pred_np),
}
sk_bic = sk_gmm.bic(X_np)
sk_aic = sk_gmm.aic(X_np)

print(&quot;\n=== Sklearn Metrics for n_components=4 ===&quot;)
for metric, val in scores_sklearn.items():
    print(f&quot;{metric}: {val:.4f}&quot;)
print(f&quot;BIC: {sk_bic:.4f}&quot;)
print(f&quot;AIC: {sk_aic:.4f}&quot;)

# Compare BIC/AIC with Torch
torch_bic = ClusteringMetrics.bic_score(gmm.lower_bound_, X_tensor, n_components, gmm.covariance_type)
torch_aic = ClusteringMetrics.aic_score(gmm.lower_bound_, X_tensor, n_components, gmm.covariance_type)
print(f&quot;\nTorch BIC: {torch_bic:.4f} vs Sklearn BIC: {sk_bic:.4f}&quot;)
print(f&quot;Torch AIC: {torch_aic:.4f} vs Sklearn AIC: {sk_aic:.4f}&quot;)
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
=== Torch Metrics for n_components=4 ===
rand_score: 0.9963
adjusted_rand_score: 0.9908
mutual_info_score: 1.3139
adjusted_mutual_info_score: 0.9852
normalized_mutual_info_score: 0.9852
fowlkes_mallows_score: 0.9934
homogeneity_score: 0.9851
completeness_score: 0.9853
v_measure_score: 0.9852
purity_score: 0.9968
silhouette_score: 0.6674
davies_bouldin_index: 0.4736
calinski_harabasz_score: 7822.9487
dunn_index: 0.0179
bic_score: 28528.9258
aic_score: 28178.6209

=== Sklearn Metrics for n_components=4 ===
rand_score: 0.9963
adjusted_rand_score: 0.9908
mutual_info_score: 1.3139
normalized_mutual_info_score: 0.9852
adjusted_mutual_info_score: 0.9852
fowlkes_mallows_score: 0.9934
homogeneity_score: 0.9851
completeness_score: 0.9853
v_measure_score: 0.9852
silhouette_score: 0.6674
davies_bouldin_index: 0.4736
calinski_harabasz_score: 7822.9492
BIC: 28528.8182
AIC: 28178.5131

Torch BIC: 28528.9258 vs Sklearn BIC: 28528.8182
Torch AIC: 28178.6209 vs Sklearn AIC: 28178.5131
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_np)  # shape (N, 2)
X_pca_tensor = torch.tensor(X_pca, dtype=torch.float32, device=device)

# Let&#39;s define a helper to project means/covariances into PCA space
def transform_means_covariances(means, covariances, pca_sklearn):
    &quot;&quot;&quot;
    Project the GMM means &amp; covariances into the PCA-reduced space.
    We&#39;ll only keep the top 2 principal components, so each covariance
    becomes 2x2 and each mean is 2D.
    &quot;&quot;&quot;
    means_cpu = means.detach().cpu().numpy()
    means_pca = pca_sklearn.transform(means_cpu)  # NxD -&gt; Nx2

    # For each covariance, do W * Cov * W^T, where W are the top 2 PC loadings.
    W = pca_sklearn.components_[:2, :]  # shape (2, original_dim)
    covariances_pca = []

    if covariances.dim() == 3:  # e.g., &#39;full&#39; shape = (n_components, d, d)
        for i in range(covariances.size(0)):
            cov_cpu = covariances[i].detach().cpu().numpy()
            cov_2d = W @ cov_cpu @ W.T
            covariances_pca.append(cov_2d)
    else:
        # For diag or spherical, adapt accordingly
        # We&#39;ll do an approximate approach for demonstration
        cov_2d = np.diag(covariances[0].cpu().numpy()[:2])
        covariances_pca = [cov_2d for _ in range(means.size(0))]

    covariances_pca = np.stack(covariances_pca, axis=0)
    return torch.tensor(means_pca, dtype=torch.float32), torch.tensor(covariances_pca, dtype=torch.float32)

means_pca, covariances_pca = transform_means_covariances(gmm.means_, gmm.covariances_, pca)

# Next, let&#39;s define a quick function to match predicted labels to the
# ground truth using the Hungarian algorithm (so that colors match).
from scipy.optimize import linear_sum_assignment

def match_labels(y_true_tensor, y_pred_tensor):
    y_true_cpu = y_true_tensor.cpu().long()
    y_pred_cpu = y_pred_tensor.cpu().long()
    max_true = y_true_cpu.max().item() + 1
    max_pred = y_pred_cpu.max().item() + 1

    # Contingency matrix
    cont = np.zeros((max_true, max_pred), dtype=int)
    for i in range(y_true_cpu.size(0)):
        cont[y_true_cpu[i], y_pred_cpu[i]] += 1

    row_ind, col_ind = linear_sum_assignment(-cont)  # maximize
    mapping = {col_ind[j]: row_ind[j] for j in range(len(row_ind))}

    matched_labels = np.array([mapping.get(p, p) for p in y_pred_cpu.numpy()], dtype=int)
    return matched_labels

matched_pred = match_labels(y_tensor, labels_pred)

# Let&#39;s visualize
fig, ax = plt.subplots(figsize=(8, 6))

correct = (matched_pred == y_tensor.cpu().numpy())
incorrect = ~correct

ax.scatter(X_pca[correct, 0], X_pca[correct, 1], c=&#39;lightblue&#39;, s=5, label=&#39;Correctly predicted&#39;)
ax.scatter(X_pca[incorrect, 0], X_pca[incorrect, 1], c=&#39;red&#39;, s=5, label=&#39;Incorrectly predicted&#39;)

# Plot the 2D means + ellipses for each component
n_comps = gmm.means_.shape[0]
for i in range(n_comps):
    mean_2d = means_pca[i].numpy()
    cov_2d = covariances_pca[i].numpy()

    ax.scatter(mean_2d[0], mean_2d[1], c=&#39;black&#39;, marker=&#39;x&#39;)
    # Ellipse for ~95% confidence region: factor ~5.991 for 2D
    eigvals, eigvecs = np.linalg.eigh(cov_2d)
    order = eigvals.argsort()[::-1]
    eigvals, eigvecs = eigvals[order], eigvecs[:, order]
    angle = np.degrees(np.arctan2(eigvecs[1, 0], eigvecs[0, 0]))
    width, height = 2 * np.sqrt(5.991 * eigvals)
    ell = Ellipse(mean_2d, width, height, angle=angle, edgecolor=&#39;black&#39;, facecolor=&#39;none&#39;, linestyle=&#39;--&#39;)
    ax.add_patch(ell)

ax.set_title(&quot;Clustering Visualization in PCA (4D -&gt; 2D)&quot;)
ax.set_xlabel(&quot;PCA 1&quot;)
ax.set_ylabel(&quot;PCA 2&quot;)
ax.legend()
plt.show()
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_metrics_5_0.png" src="../_images/notebooks_metrics_5_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>cm = ClusteringMetrics.confusion_matrix(
    torch.tensor(y_true, dtype=torch.long),
    torch.tensor(matched_pred, dtype=torch.long)
)
print(&quot;\nConfusion Matrix:\n&quot;, cm)

report = ClusteringMetrics.classification_report(
    torch.tensor(y_true, dtype=torch.long),
    torch.tensor(matched_pred, dtype=torch.long)
)
print(&quot;\nClassification Report:\n&quot;, report)

# Heatmap of confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt=&#39;d&#39;, cmap=&#39;Blues&#39;)
plt.title(&quot;Confusion Matrix (Hungarian-Aligned Labels)&quot;)
plt.xlabel(&quot;Predicted Label&quot;)
plt.ylabel(&quot;True Label&quot;)
plt.show()

# Convert classification report to a DataFrame for display
report_df = pd.DataFrame(report).T
report_df
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

Confusion Matrix:
 tensor([[996,   0,   0,   4],
        [  0, 800,   0,   0],
        [  0,   0, 400,   0],
        [  5,   0,   0, 595]], dtype=torch.int32)

Classification Report:
 {0: {&#39;precision&#39;: 0.995004995004995, &#39;recall&#39;: 0.996, &#39;f1-score&#39;: 0.9955022488755622, &#39;support&#39;: 1000, &#39;jaccard&#39;: 0.991044776119403, &#39;roc_auc&#39;: 0.99549400806427}, 1: {&#39;precision&#39;: 1.0, &#39;recall&#39;: 1.0, &#39;f1-score&#39;: 1.0, &#39;support&#39;: 800, &#39;jaccard&#39;: 1.0, &#39;roc_auc&#39;: 1.0}, 2: {&#39;precision&#39;: 1.0, &#39;recall&#39;: 1.0, &#39;f1-score&#39;: 1.0, &#39;support&#39;: 400, &#39;jaccard&#39;: 1.0, &#39;roc_auc&#39;: 1.0}, 3: {&#39;precision&#39;: 0.993322203672788, &#39;recall&#39;: 0.9916666666666667, &#39;f1-score&#39;: 0.9924937447873228, &#39;support&#39;: 600, &#39;jaccard&#39;: 0.9850993377483444, &#39;roc_auc&#39;: 0.9971325993537903}}
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_metrics_6_1.png" src="../_images/notebooks_metrics_6_1.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>precision</th>
      <th>recall</th>
      <th>f1-score</th>
      <th>support</th>
      <th>jaccard</th>
      <th>roc_auc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.995005</td>
      <td>0.996000</td>
      <td>0.995502</td>
      <td>1000.0</td>
      <td>0.991045</td>
      <td>0.995494</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>800.0</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>400.0</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.993322</td>
      <td>0.991667</td>
      <td>0.992494</td>
      <td>600.0</td>
      <td>0.985099</td>
      <td>0.997133</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>RUN_KL_DIVERGENCE = True
if RUN_KL_DIVERGENCE:
    print(&quot;Computing KL(p||q) for different numbers of components...&quot;)
    gmm_true = GaussianMixture(
        n_features=4,
        n_components=true_n_components,
        covariance_type=&#39;full&#39;,
        max_iter=1000,
        init_params=&#39;kmeans&#39;,
        device=device
    )
    gmm_true.fit(X_tensor)

    test_range = np.arange(5, 15)
    kl_vals = torch.zeros(len(test_range), device=device)
    for i, n in tqdm(enumerate(test_range), total=len(test_range)):
        gmm_test = GaussianMixture(
            n_features=4,
            n_components=n,
            covariance_type=&#39;full&#39;,
            max_iter=1000,
            init_params=&#39;kmeans&#39;,
            device=device
        )
        gmm_test.fit(X_tensor)

        kl_vals[i] = ClusteringMetrics.kl_divergence_gmm(gmm_true, gmm_test, n_samples=10000)

    plt.figure(figsize=(8, 5))
    plt.plot(test_range, kl_vals.cpu().numpy(), marker=&#39;o&#39;)
    plt.yscale(&#39;log&#39;)
    plt.title(&quot;KL Divergence: True GMM vs. Various n_components&quot;)
    plt.xlabel(&quot;Number of Components&quot;)
    plt.ylabel(&quot;KL Divergence (log scale)&quot;)
    plt.grid(True)
    plt.show()
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Computing KL(p||q) for different numbers of components...
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:01&lt;00:00,  7.98it/s]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_metrics_7_2.png" src="../_images/notebooks_metrics_7_2.png" />
</div>
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="gmm.html" class="btn btn-neutral float-left" title="Tutorial: A Gaussian Mixture Model" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="priors.html" class="btn btn-neutral float-right" title="Priors" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Adri√°n A. Sousa-Poza.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>