

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Priors &mdash; TorchGMM 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=01f34227"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Metrics" href="metrics.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            TorchGMM
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../source/modules.html">Modules</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../source/tutorials.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="gmm.html">Tutorial: A Gaussian Mixture Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics.html">Metrics</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Priors</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">TorchGMM</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../source/tutorials.html">Tutorials</a></li>
      <li class="breadcrumb-item active">Priors</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/notebooks/priors.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Priors">
<h1>Priors<a class="headerlink" href="#Priors" title="Link to this heading">ÔÉÅ</a></h1>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import torch
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
from matplotlib.colors import ListedColormap
import importlib
import os
import sys
sys.path.append(&#39;../../..&#39;)
#print(os.listdir(&quot;../../..&quot;))

import utils.gmm
import utils.metrics
import utils.gmm_init
importlib.reload(utils.gmm)
importlib.reload(utils.metrics)
importlib.reload(utils.gmm_init)
from utils.gmm import GaussianMixture
from utils.gmm_init import GMMInitializer

device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
print(&quot;Using device:&quot;, device)

# Set random seeds for reproducibility
random_state = 42
np.random.seed(random_state)
torch.manual_seed(random_state)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Using device: cuda
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;torch._C.Generator at 0x7fcb8c2fb2d0&gt;
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n_features = 2
n_components = 4
max_iter = 1000

n_samples_1 = 1000
n_samples_2 = 500
n_samples_3 = 200
n_samples_4 = 300

center_1 = np.array([0, 0])
center_2 = np.array([-4, 4])
center_3 = np.array([4, -4])
center_4 = np.array([4, 4])

C = np.array([[0.0, -0.5], [1.5, 0.5]])
C_2 = np.array([[0.0, 0.2], [0.4, 1.7]])

component_1 = np.dot(np.random.randn(n_samples_1, 2), C) + center_1
component_2 = 0.7 * np.random.randn(n_samples_2, 2) + center_2
component_3 = 0.5 * np.random.randn(n_samples_3, 2) + center_3
component_4 = np.dot(np.random.randn(n_samples_4, 2), C_2) + center_4

X = np.concatenate([component_1, component_2, component_3, component_4], axis=0)
X_tensor = torch.tensor(X, dtype=torch.float32, device=device)
<br/></pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def plot_gmm_ellipses(gmm, X, ax=None, title=&#39;&#39;, color=&#39;blue&#39;, alpha=1.0):
    &quot;&quot;&quot;
    Plot a scatter of all data points and overlay ellipses for each component
    in the GMM, along with the component means.
    &quot;&quot;&quot;
    if ax is None:
        ax = plt.gca()

    ax.scatter(X[:, 0].cpu(), X[:, 1].cpu(), c=&#39;black&#39;, s=2, alpha=0.3, label=&#39;Data&#39;)

    for k in range(gmm.n_components):
        mean_k = gmm.means_[k].detach().cpu().numpy()

        # Extract appropriate covariance
        if gmm.covariance_type == &#39;full&#39;:
            cov_k = gmm.covariances_[k].detach().cpu().numpy()
        elif gmm.covariance_type == &#39;diag&#39;:
            cov_k = np.diag(gmm.covariances_[k].detach().cpu().numpy())
        elif gmm.covariance_type == &#39;spherical&#39;:
            var = gmm.covariances_[k].detach().cpu().item()
            cov_k = np.eye(gmm.n_features) * var
        elif gmm.covariance_type == &#39;tied_full&#39;:
            cov_k = gmm.covariances_.detach().cpu().numpy()  # one shared covariance
        elif gmm.covariance_type == &#39;tied_diag&#39;:
            diag_vals = gmm.covariances_.detach().cpu().numpy()
            cov_k = np.diag(diag_vals)
        elif gmm.covariance_type == &#39;tied_spherical&#39;:
            var = gmm.covariances_.detach().cpu().item()
            cov_k = np.eye(gmm.n_features) * var
        else:
            raise ValueError(&quot;Unsupported covariance type.&quot;)

        # Decompose covariance to plot ellipse
        eigvals, eigvecs = np.linalg.eigh(cov_k)
        order = eigvals.argsort()[::-1]
        eigvals, eigvecs = eigvals[order], eigvecs[:, order]
        angle = np.degrees(np.arctan2(*eigvecs[:, 0][::-1]))

        # For example, 2 std dev =&gt; coverage ~95%
        std_dev = 2
        width, height = 2 * std_dev * np.sqrt(eigvals)
        ellipse = Ellipse(
            mean_k, width, height, angle=angle,
            edgecolor=color, facecolor=&#39;none&#39;, linewidth=2, alpha=alpha
        )
        ax.add_patch(ellipse)

        # Mark the mean
        ax.plot(mean_k[0], mean_k[1], &#39;o&#39;, color=color, markeredgecolor=&#39;black&#39;)

    ax.set_title(title)
    ax.axis(&#39;equal&#39;)
    ax.legend()
<br/></pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>weight_concentration_values = [
    torch.tensor([1, 1, 1, 1], device=device),
    torch.tensor([100.0, 100.0, 100.0, 100.0], device=device),
    torch.tensor([10000.0, 10000.0, 10000.0, 10000.0], device=device)
]

fig, axs = plt.subplots(1, 3, figsize=(15, 5))
fig.suptitle(&quot;Weight Priors: Colored by Weights&quot;)

colors = [&#39;red&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;purple&#39;]

for ax, alpha_vec in zip(axs, weight_concentration_values):
    gmm_weight_prior = GaussianMixture(
        n_features=n_features,
        n_components=n_components,
        covariance_type=&#39;full&#39;,
        max_iter=1000,
        device=device,
        weight_concentration_prior=alpha_vec,
        random_state=random_state
    )
    gmm_weight_prior.fit(X_tensor)

    # Plot
    ax.scatter(X[:, 0], X[:, 1], c=&#39;black&#39;, s=2, alpha=0.2)
    for i, base_color in enumerate(colors):
        w_i = float(gmm_weight_prior.weights_[i].cpu().item())
        # Retrieve mean, covariance
        mean_i = gmm_weight_prior.means_[i].cpu().numpy()
        cov_i = gmm_weight_prior.covariances_[i].cpu().numpy()

        eigvals, eigvecs = np.linalg.eigh(cov_i)
        order = eigvals.argsort()[::-1]
        eigvals, eigvecs = eigvals[order], eigvecs[:, order]
        angle = np.degrees(np.arctan2(*eigvecs[:, 0][::-1]))
        width, height = 2.0 * 2.0 * np.sqrt(eigvals)  # e.g. 2 std dev

        ell = Ellipse(mean_i, width, height, angle=angle,
                      edgecolor=base_color, facecolor=base_color,
                      alpha=w_i, label=f&#39;Comp {i}, w={w_i:.2f}&#39;)
        ax.add_patch(ell)

    # Label each subplot with prior strength and final LL
    ax.set_title(f&quot;alpha={alpha_vec[0].item()}\nLL={gmm_weight_prior.lower_bound_:.4f}&quot;)
    ax.axis(&#39;equal&#39;)
    ax.legend(loc=&#39;upper right&#39;)

plt.tight_layout()
plt.show()
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_priors_4_0.png" src="../_images/notebooks_priors_4_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>init_means_kmeans = GMMInitializer.kmeans(X_tensor.cpu(), k=n_components)  # do on CPU
mean_prior = init_means_kmeans.clone().to(device)
mean_precision_prior = 1e5  # large =&gt; strong push toward prior means

# perturb the prior means slightly
mean_prior += 0.5 * torch.randn_like(mean_prior)

gmm_mean_prior = GaussianMixture(
    n_features=n_features,
    n_components=n_components,
    covariance_type=&#39;full&#39;,
    max_iter=max_iter,
    device=device,
    mean_prior=mean_prior,
    mean_precision_prior=mean_precision_prior,
    random_state=random_state
)
gmm_mean_prior.fit(X_tensor)

fig, ax = plt.subplots(1, 1, figsize=(6, 6))
title = f&quot;Mean Prior (precision={mean_precision_prior})\nLL={gmm_mean_prior.lower_bound_:.2f}&quot;
plot_gmm_ellipses(gmm_mean_prior, X_tensor, ax=ax, title=title, color=&#39;green&#39;)

# Plot the prior means as red crosses
ax.scatter(mean_prior[:, 0].cpu(), mean_prior[:, 1].cpu(), marker=&#39;x&#39;, s=100, c=&#39;red&#39;, label=&#39;Mean Prior&#39;)
ax.legend()
plt.tight_layout()
plt.show()
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_priors_5_0.png" src="../_images/notebooks_priors_5_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>data_covariance = np.cov(X_tensor.cpu().numpy(), rowvar=False)  # shape (2, 2)
data_covariance = torch.tensor(data_covariance, dtype=torch.float32, device=device)
data_covariance = data_covariance.unsqueeze(0).expand(n_components, -1, -1)

degrees_of_freedom_prior = float(n_features + 2)
prior_strengths = [0.01, 0.1, 1.0]

fig, axs = plt.subplots(1, 3, figsize=(15, 5))
fig.suptitle(&quot;Covariance Priors: Varying Strength&quot;)

for ax, strength in zip(axs, prior_strengths):
    cov_prior = data_covariance * strength

    dummy_mean_prior = torch.zeros(n_components, n_features, device=device)
    dummy_mean_precision_prior = 1e-10  # effectively no strong push on means

    gmm_cov_prior = GaussianMixture(
        n_features=n_features,
        n_components=n_components,
        covariance_type=&#39;full&#39;,
        max_iter=max_iter,
        device=device,
        # Covariance prior:
        covariance_prior=cov_prior,
        degrees_of_freedom_prior=degrees_of_freedom_prior,
        # Provide &quot;dummy&quot; mean priors so code doesn&#39;t crash
        mean_prior=dummy_mean_prior,
        mean_precision_prior=dummy_mean_precision_prior
    )
    gmm_cov_prior.fit(X_tensor)


    title = (f&quot;CovPrior Strength={strength}\n&quot;
             f&quot;dof={degrees_of_freedom_prior}\nLogLik={gmm_cov_prior.lower_bound_:.2f}&quot;)
    plot_gmm_ellipses(gmm_cov_prior, X_tensor, ax=ax, title=title, color=&#39;magenta&#39;)

plt.tight_layout()
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_priors_6_0.png" src="../_images/notebooks_priors_6_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>dof_values = [10.0, 100.0, 1000.0]  # must be &gt; n_features - 1 =&gt; &gt; 1 for 2D
strength = 0.1  # fixed prior strength for demonstration

fig, axs = plt.subplots(1, 3, figsize=(15, 5))
fig.suptitle(&quot;Covariance Prior: Varying Degrees of Freedom&quot;)

for ax, dof in zip(axs, dof_values):
    cov_prior = data_covariance * strength

    # Example to skip MAP on means by making them &quot;neutral&quot;
    dummy_mean_prior = torch.zeros(n_components, n_features, device=device)
    dummy_mean_precision_prior = 1e-10  # effectively no strong push on means

    gmm_cov_prior = GaussianMixture(
        n_features=n_features,
        n_components=n_components,
        covariance_type=&#39;full&#39;,
        max_iter=max_iter,
        device=device,
        # Covariance prior:
        covariance_prior=cov_prior,
        degrees_of_freedom_prior=dof,
        # Provide &quot;dummy&quot; mean priors so code doesn&#39;t crash
        mean_prior=dummy_mean_prior,
        mean_precision_prior=dummy_mean_precision_prior,
        random_state=random_state
    )
    gmm_cov_prior.fit(X_tensor)


    title = (f&quot;DOF={dof}, Strength={strength}\n&quot;
             f&quot;LL={gmm_cov_prior.lower_bound_:.4f}&quot;)
    plot_gmm_ellipses(gmm_cov_prior, X_tensor, ax=ax, title=title, color=&#39;green&#39;)

plt.tight_layout()
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_priors_7_0.png" src="../_images/notebooks_priors_7_0.png" />
</div>
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="metrics.html" class="btn btn-neutral float-left" title="Metrics" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Adri√°n A. Sousa-Poza.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>