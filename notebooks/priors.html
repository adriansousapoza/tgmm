

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Priors in Gaussian Mixture Models &mdash; TorchGMM 0.1.7 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=5c7ff671" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=ca7ad2ea"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "displayMath": [["$$", "$$"], ["\\[", "\\]"]]}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="TorchGMM Plotting Function Comprehensive Demo" href="visualise.html" />
    <link rel="prev" title="Clustering Metrics: Comprehensive Evaluation of Gaussian Mixture Models" href="metrics.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            TorchGMM
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../source/modules.html">Modules</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../source/tutorials.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="gmm.html">Gaussian Mixture Model (GMM) Usage, Analysis and Visualization</a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics.html">Clustering Metrics: Comprehensive Evaluation of Gaussian Mixture Models</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Priors in Gaussian Mixture Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Synthetic-Data-Generation">Synthetic Data Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Dirichlet-Distribution:-Definition-and-Derivation-from-the-Beta-Distribution">Dirichlet Distribution: Definition and Derivation from the Beta Distribution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Derivation-from-the-Beta-Distribution">Derivation from the Beta Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Symmetric-Dirichlet-Distribution">Symmetric Dirichlet Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#How-Does-\alpha-Affect-the-Dirichlet-Distribution?">How Does <span class="math notranslate nohighlight">\(\alpha\)</span> Affect the Dirichlet Distribution?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Weight-Priors-in-GMM:-Small-vs.-Large-\alpha">Weight Priors in GMM: Small vs. Large <span class="math notranslate nohighlight">\(\alpha\)</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#Unbalanced-Weight-Priors:-Dominant-Component-Effect">Unbalanced Weight Priors: Dominant Component Effect</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Prior-on-the-Means-in-Gaussian-Mixture-Models">Prior on the Means in Gaussian Mixture Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Gaussian-Prior-on-the-Means">Gaussian Prior on the Means</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Impact-of-the-Mean-Precision-Prior">Impact of the Mean Precision Prior</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#The-Gamma-Distribution-and-its-Inverse">The Gamma Distribution and its Inverse</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Mean-of-the-Inverse-Gamma-Distribution">Mean of the Inverse Gamma Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Impact-of-the-Parameters">Impact of the Parameters</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Visualizing-the-Inverse-Gamma-Distribution">Visualizing the Inverse Gamma Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#The-Wishart-Distribution-and-its-Inverse">The Wishart Distribution and its Inverse</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#From-the-Gamma-to-the-Wishart-Distribution">From the Gamma to the Wishart Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Why-Use-the-Inverse-Wishart-and-Inverse-Gamma?">Why Use the Inverse Wishart and Inverse Gamma?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Covariance-Priors-in-GMM">Covariance Priors in GMM</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#1.-Varying-Prior-Strength">1. Varying Prior Strength</a></li>
<li class="toctree-l4"><a class="reference internal" href="#2.-Varying-Degrees-of-Freedom-(DOF)">2. Varying Degrees of Freedom (DOF)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#3.-Varying-Degrees-of-Freedom-(DOF)">3. Varying Degrees of Freedom (DOF)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="visualise.html">TorchGMM Plotting Function Comprehensive Demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="cem.html">Classification Expectation Maximization (CEM) Algorithm</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">TorchGMM</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../source/tutorials.html">Tutorials</a></li>
      <li class="breadcrumb-item active">Priors in Gaussian Mixture Models</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <hr class="docutils" />
<section id="Priors-in-Gaussian-Mixture-Models">
<h1>Priors in Gaussian Mixture Models<a class="headerlink" href="#Priors-in-Gaussian-Mixture-Models" title="Link to this heading"></a></h1>
<p>In this notebook, we explore how to incorporate <strong>priors</strong> into a Gaussian Mixture Model (GMM) using the <code class="docutils literal notranslate"><span class="pre">tgmm</span></code> package. Priors help regularize model parameters and can steer the Expectation-Maximization (EM) algorithm toward more robust, stable solutions.</p>
<p>We will illustrate the use of three types of priors:</p>
<ul class="simple">
<li><p><strong>Weight Priors:</strong> Dirichlet priors on the mixture component weights.</p></li>
<li><p><strong>Mean Priors:</strong> Gaussian priors on the component means.</p></li>
<li><p><strong>Covariance Priors:</strong> Inverse-Wishart (or related) priors on the covariance matrices.</p></li>
</ul>
<p>Let’s begin by setting up our environment and generating some synthetic data.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[42]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">scipy.special</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">wishart</span><span class="p">,</span> <span class="n">invgamma</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.ndimage</span><span class="w"> </span><span class="kn">import</span> <span class="n">gaussian_filter</span>

<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="s1">&#39;../&#39;</span><span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">tgmm</span><span class="w"> </span><span class="kn">import</span> <span class="n">GaussianMixture</span><span class="p">,</span> <span class="n">GMMInitializer</span><span class="p">,</span> <span class="n">dynamic_figsize</span><span class="p">,</span> <span class="n">plot_gmm</span>

<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;CUDA version:&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Device:&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Using CPU&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CUDA version: 12.4
Device: NVIDIA GeForce RTX 4060 Laptop GPU
</pre></div></div>
</div>
<hr class="docutils" />
<section id="Synthetic-Data-Generation">
<h2>Synthetic Data Generation<a class="headerlink" href="#Synthetic-Data-Generation" title="Link to this heading"></a></h2>
<p>The synthetic dataset is generated by combining four Gaussian components:</p>
<ul class="simple">
<li><p><strong>Component 1:</strong> Centered at <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">2]</span></code> with spherical covariance.</p></li>
<li><p><strong>Component 2:</strong> Centered at <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">-2]</span></code> with spherical covariance (fewer points).</p></li>
<li><p><strong>Component 3:</strong> Centered at <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">0]</span></code> with diagonal covariance.</p></li>
<li><p><strong>Component 4:</strong> Centered at <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">2]</span></code> with full covariance.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[43]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_samples</span> <span class="o">=</span> <span class="p">[</span><span class="mi">800</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]</span>
<span class="n">centers</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>
           <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]),</span>
           <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span>
           <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])]</span>
<span class="n">covs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mf">1.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>                    <span class="c1"># spherical covariance</span>
    <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>                    <span class="c1"># spherical covariance, fewer points</span>
    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]]),</span>       <span class="c1"># diagonal covariance</span>
    <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>    <span class="c1"># full covariance</span>
<span class="p">]</span>

<span class="n">components</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">center</span><span class="p">,</span> <span class="n">cov</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">centers</span><span class="p">,</span> <span class="n">covs</span><span class="p">):</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">cov</span><span class="p">)</span> <span class="o">+</span> <span class="n">center</span>
    <span class="n">components</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">components</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">i</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)])</span>
<span class="n">legend_labels</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;Component </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))]</span>

<span class="n">X_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">n_components</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>


<span class="n">plot_gmm</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">true_labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Original Data&#39;</span><span class="p">,</span> <span class="n">legend_labels</span><span class="o">=</span><span class="n">legend_labels</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_priors_3_0.png" src="../_images/notebooks_priors_3_0.png" />
</div>
</div>
</section>
<hr class="docutils" />
<section id="Dirichlet-Distribution:-Definition-and-Derivation-from-the-Beta-Distribution">
<h2>Dirichlet Distribution: Definition and Derivation from the Beta Distribution<a class="headerlink" href="#Dirichlet-Distribution:-Definition-and-Derivation-from-the-Beta-Distribution" title="Link to this heading"></a></h2>
<p>The Dirichlet distribution is a multivariate generalization of the Beta distribution. For a vector</p>
<div class="math notranslate nohighlight">
\[\mathbf{x} = (x_1, x_2, \dots, x_K)\]</div>
<p>that lies in the <span class="math notranslate nohighlight">\((K-1)\)</span>-simplex, i.e.,</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^K x_i = 1 \quad \text{and} \quad x_i \ge 0 \quad \text{for all } i,\]</div>
<p>the Dirichlet distribution with parameter vector</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\alpha} = (\alpha_1, \alpha_2, \dots, \alpha_K) \quad \text{with} \quad \alpha_i &gt; 0,\]</div>
<p>is defined by its probability density function</p>
<div class="math notranslate nohighlight">
\[f(x_1, \dots, x_K; \boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})} \prod_{i=1}^K x_i^{\alpha_i - 1},\]</div>
<p>where the normalizing constant <span class="math notranslate nohighlight">\(B(\boldsymbol{\alpha})\)</span> is the multivariate Beta function:</p>
<div class="math notranslate nohighlight">
\[B(\boldsymbol{\alpha}) = \frac{\prod_{i=1}^K \Gamma(\alpha_i)}{\Gamma\left(\sum_{i=1}^K \alpha_i\right)}.\]</div>
<section id="Derivation-from-the-Beta-Distribution">
<h3>Derivation from the Beta Distribution<a class="headerlink" href="#Derivation-from-the-Beta-Distribution" title="Link to this heading"></a></h3>
<p>For the special case <span class="math notranslate nohighlight">\(K=2\)</span>, let</p>
<div class="math notranslate nohighlight">
\[x_1 = x \quad \text{and} \quad x_2 = 1 - x.\]</div>
<p>Then the Dirichlet density becomes</p>
<div class="math notranslate nohighlight">
\[f(x; \alpha_1, \alpha_2) = \frac{1}{B(\alpha_1, \alpha_2)} x^{\alpha_1 - 1} (1 - x)^{\alpha_2 - 1},\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[B(\alpha_1, \alpha_2) = \frac{\Gamma(\alpha_1)\Gamma(\alpha_2)}{\Gamma(\alpha_1 + \alpha_2)}.\]</div>
<p>This is exactly the density of the Beta distribution, denoted as</p>
<div class="math notranslate nohighlight">
\[\operatorname{Beta}(\alpha_1,\alpha_2).\]</div>
</section>
<section id="Symmetric-Dirichlet-Distribution">
<h3>Symmetric Dirichlet Distribution<a class="headerlink" href="#Symmetric-Dirichlet-Distribution" title="Link to this heading"></a></h3>
<p>In its most general form, the Dirichlet distribution has a parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} = (\alpha_1, \alpha_2, \dots, \alpha_K)\)</span>, with each <span class="math notranslate nohighlight">\(\alpha_i\)</span> playing a role similar to that in the Beta distribution. However, in many practical applications—especially when there is no prior reason to favor one category over another—a <strong>symmetric Dirichlet distribution</strong> is used. In the symmetric case, all parameters are set equal, i.e.,</p>
<div class="math notranslate nohighlight">
\[\alpha_1 = \alpha_2 = \cdots = \alpha_K = \alpha.\]</div>
<p>This reduces the number of free parameters from <span class="math notranslate nohighlight">\(K\)</span> to one, and the density function simplifies to</p>
<div class="math notranslate nohighlight">
\[f(x_1,\dots,x_K; \alpha) = \frac{\Gamma(K\alpha)}{[\Gamma(\alpha)]^K} \prod_{i=1}^K x_i^{\alpha - 1}.\]</div>
<p>Thus, while the general Dirichlet has <span class="math notranslate nohighlight">\(K\)</span> parameters, the symmetric case is completely described by a single concentration parameter <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
</section>
<section id="How-Does-\alpha-Affect-the-Dirichlet-Distribution?">
<h3>How Does <span class="math notranslate nohighlight">\(\alpha\)</span> Affect the Dirichlet Distribution?<a class="headerlink" href="#How-Does-\alpha-Affect-the-Dirichlet-Distribution?" title="Link to this heading"></a></h3>
<ul>
<li><div class="line-block">
<div class="line"><strong>Small :math:`alpha` (:math:`alpha ll 1`):</strong></div>
<div class="line">When <span class="math notranslate nohighlight">\(\alpha\)</span> is much less than 1, the distribution is <em>sparse</em>, favoring configurations where one or a few weights are near 1 while the others are close to 0. This tends to lead to “hard” clustering where only a few components dominate.</div>
</div>
</li>
<li><div class="line-block">
<div class="line"><strong>Uniform Case (:math:`alpha = 1`):</strong></div>
<div class="line">For <span class="math notranslate nohighlight">\(\alpha = 1\)</span>, the Dirichlet distribution is uniform over the simplex, meaning that all weight configurations are equally likely a priori.</div>
</div>
</li>
<li><div class="line-block">
<div class="line"><strong>Large :math:`alpha` (:math:`alpha gg 1`):</strong></div>
<div class="line">With a large <span class="math notranslate nohighlight">\(\alpha\)</span>, the distribution becomes more concentrated around the center of the simplex, promoting balanced weights across all components.</div>
</div>
</li>
</ul>
<p>The plots below visualize how different <span class="math notranslate nohighlight">\(\alpha\)</span> values affect the Dirichlet density over a ternary plot.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[44]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">mpltern</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">mpltern</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">mpltern</span>

<span class="k">def</span><span class="w"> </span><span class="nf">dirichlet_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the Dirichlet density for point (x, y, z)</span>
<span class="sd">    given parameters alpha = (α1, α2, α3).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">,</span> <span class="n">a3</span> <span class="o">=</span> <span class="n">alpha</span>
    <span class="n">B</span> <span class="o">=</span> <span class="p">(</span><span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span> <span class="o">*</span> <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span> <span class="o">*</span> <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">a3</span><span class="p">)</span> <span class="o">/</span>
         <span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">a1</span> <span class="o">+</span> <span class="n">a2</span> <span class="o">+</span> <span class="n">a3</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="p">(</span><span class="n">a1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span><span class="o">**</span><span class="p">(</span><span class="n">a2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">z</span><span class="o">**</span><span class="p">(</span><span class="n">a3</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">B</span>

<span class="n">alpha_list</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s2">&quot;Dirichlet Density (α=2,2,2)&quot;</span><span class="p">),</span>
    <span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="s2">&quot;Dirichlet Density (α=8,8,8)&quot;</span><span class="p">),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s2">&quot;Dirichlet Density (α=1,1,2)&quot;</span><span class="p">),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s2">&quot;Dirichlet Density (α=1,2,2)&quot;</span><span class="p">),</span>
    <span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="s2">&quot;Dirichlet Density (α=2,4,8)&quot;</span><span class="p">),</span>
    <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="s2">&quot;Dirichlet Density (α=1,1,1)&quot;</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">a_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.9999</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">b_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.9999</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">A</span><span class="p">,</span> <span class="n">B_mesh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">a_vals</span><span class="p">,</span> <span class="n">b_vals</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">A</span> <span class="o">-</span> <span class="n">B_mesh</span>

<span class="n">mask</span> <span class="o">=</span> <span class="n">C</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span class="n">x_grid</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
<span class="n">y_grid</span> <span class="o">=</span> <span class="n">B_mesh</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
<span class="n">z_grid</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>

<span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">ncols</span><span class="p">,</span>
                         <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;projection&#39;</span><span class="p">:</span> <span class="s1">&#39;ternary&#39;</span><span class="p">},</span>
                         <span class="n">figsize</span><span class="o">=</span><span class="n">dynamic_figsize</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">title</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">,</span> <span class="n">alpha_list</span><span class="p">):</span>
    <span class="n">densities</span> <span class="o">=</span> <span class="n">dirichlet_pdf</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">y_grid</span><span class="p">,</span> <span class="n">z_grid</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

    <span class="c1"># Draw the simplex boundary.</span>
    <span class="n">vertices</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vertices</span><span class="p">],</span>
            <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vertices</span><span class="p">],</span>
            <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vertices</span><span class="p">],</span>
            <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Plot filled contours using the clipped densities.</span>
    <span class="n">triang</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">tricontourf</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">y_grid</span><span class="p">,</span> <span class="n">z_grid</span><span class="p">,</span> <span class="n">densities</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">tricontour</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">y_grid</span><span class="p">,</span> <span class="n">z_grid</span><span class="p">,</span> <span class="n">densities</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">triang</span><span class="o">.</span><span class="n">levels</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Different Dirichlet Distributions&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_priors_5_0.png" src="../_images/notebooks_priors_5_0.png" />
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="Weight-Priors-in-GMM:-Small-vs.-Large-\alpha">
<h2>Weight Priors in GMM: Small vs. Large <span class="math notranslate nohighlight">\(\alpha\)</span><a class="headerlink" href="#Weight-Priors-in-GMM:-Small-vs.-Large-\alpha" title="Link to this heading"></a></h2>
<p>Weight priors directly influence the mixing proportions of the GMM. Here, we compare the effects of setting a very small versus a very large <span class="math notranslate nohighlight">\(\alpha\)</span> value:</p>
<ul class="simple">
<li><p><strong>Small :math:`alpha`:</strong> Leads to a model where few components dominate.</p></li>
<li><p><strong>Large :math:`alpha`:</strong> Encourages the model to assign roughly equal weights to all components.</p></li>
</ul>
<p>Each subplot shows the fitted GMM components (ellipses and means) along with the log-likelihood (LL) achieved.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[45]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">weigths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e4</span><span class="p">])</span>
<span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">weight</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">weigths</span><span class="p">]</span>

<span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">figsize</span> <span class="o">=</span> <span class="n">dynamic_figsize</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">ncols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Different Priors: Small vs Large Weight&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="n">weigths</span><span class="p">):</span>
    <span class="n">gmm_weight_prior</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span>
        <span class="n">n_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
        <span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span>
        <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">weight_concentration_prior</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="n">init_params</span><span class="o">=</span><span class="s1">&#39;random&#39;</span>
    <span class="p">)</span>
    <span class="n">gmm_weight_prior</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">)</span>
    <span class="n">plot_gmm</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">gmm</span><span class="o">=</span><span class="n">gmm_weight_prior</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
             <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;alpha=</span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">LL=</span><span class="si">{</span><span class="n">gmm_weight_prior</span><span class="o">.</span><span class="n">lower_bound_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
             <span class="n">scale_alpha_by_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">ellipse_line_style</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span>
             <span class="n">ellipse_alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
             <span class="n">scale_size_by_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">ellipse_colors</span><span class="o">=</span><span class="s1">&#39;yellow&#39;</span><span class="p">,</span>
             <span class="n">ellipse_std_devs</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
             <span class="n">mean_color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>
             <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/><br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_priors_7_0.png" src="../_images/notebooks_priors_7_0.png" />
</div>
</div>
</section>
<hr class="docutils" />
<section id="Unbalanced-Weight-Priors:-Dominant-Component-Effect">
<h2>Unbalanced Weight Priors: Dominant Component Effect<a class="headerlink" href="#Unbalanced-Weight-Priors:-Dominant-Component-Effect" title="Link to this heading"></a></h2>
<p>In this example, we apply unbalanced weight priors by assigning one component a significantly higher concentration parameter than the others. As expected, the dominant component tends to capture a larger share of the data, which is reflected in both the clustering results and the overall model log-likelihood.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[46]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example: One dominant prior and three nearly zero priors.</span>
<span class="c1"># Here, we expect the first component to dominate.</span>
<span class="n">weight1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1000.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">weight2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">weight3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">weight1</span><span class="p">,</span> <span class="n">weight2</span><span class="p">,</span> <span class="n">weight3</span><span class="p">]</span>

<span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="mi">1</span>
<span class="n">figsize</span> <span class="o">=</span> <span class="n">dynamic_figsize</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">ncols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Different Priors: Unbalanced Weights&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="n">gmm_weight_prior</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span>
        <span class="n">n_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
        <span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span>
        <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">weight_concentration_prior</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="n">init_params</span><span class="o">=</span><span class="s1">&#39;random&#39;</span>
    <span class="p">)</span>
    <span class="n">gmm_weight_prior</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">)</span>
    <span class="c1"># Plot only ellipses and means, with alpha based on component weights.</span>
    <span class="n">plot_gmm</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">gmm</span><span class="o">=</span><span class="n">gmm_weight_prior</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
             <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;alpha=</span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">LL=</span><span class="si">{</span><span class="n">gmm_weight_prior</span><span class="o">.</span><span class="n">lower_bound_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
             <span class="n">scale_alpha_by_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">scale_size_by_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">ellipse_colors</span><span class="o">=</span><span class="s1">&#39;yellow&#39;</span><span class="p">,</span>
             <span class="n">ellipse_std_devs</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
             <span class="n">ellipse_line_style</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span>
             <span class="n">ellipse_alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
             <span class="n">mean_color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span>
             <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">rect</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.98</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_priors_9_0.png" src="../_images/notebooks_priors_9_0.png" />
</div>
</div>
</section>
<hr class="docutils" />
<section id="Prior-on-the-Means-in-Gaussian-Mixture-Models">
<h2>Prior on the Means in Gaussian Mixture Models<a class="headerlink" href="#Prior-on-the-Means-in-Gaussian-Mixture-Models" title="Link to this heading"></a></h2>
<p>In our Gaussian Mixture Model (GMM), we use a <strong>Gaussian prior</strong> on the component means to regularize the model and to guide the parameter estimation during Maximum a Posteriori (MAP) inference. This prior prevents the estimated means from drifting too far away from a reasonable central location, which is particularly useful when the data is noisy or scarce.</p>
<section id="Gaussian-Prior-on-the-Means">
<h3>Gaussian Prior on the Means<a class="headerlink" href="#Gaussian-Prior-on-the-Means" title="Link to this heading"></a></h3>
<p>We assume that each component mean, <span class="math notranslate nohighlight">\(\mu_k\)</span>, has a Gaussian (normal) prior of the form</p>
<div class="math notranslate nohighlight">
\[p(\mu_k) = \mathcal{N}(\mu_k; \mu_0, \Lambda_0^{-1}),\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu_0\)</span> is the <strong>prior mean</strong> (often set using an initialization procedure such as K-means),</p></li>
<li><p><span class="math notranslate nohighlight">\(\Lambda_0\)</span> is the <strong>prior precision matrix</strong> (the inverse of the prior covariance).</p></li>
</ul>
<p>For simplicity, it is common to assume that the precision matrix is a scalar multiple of the identity matrix:</p>
<div class="math notranslate nohighlight">
\[\Lambda_0 = \lambda I,\]</div>
<p>which implies that the prior treats every feature equally and independently.</p>
</section>
<section id="Impact-of-the-Mean-Precision-Prior">
<h3>Impact of the Mean Precision Prior<a class="headerlink" href="#Impact-of-the-Mean-Precision-Prior" title="Link to this heading"></a></h3>
<p>The influence of the mean prior is governed by the scalar precision parameter <span class="math notranslate nohighlight">\(\lambda\)</span> (denoted in our code as <code class="docutils literal notranslate"><span class="pre">mean_precision_prior</span></code>):</p>
<ul>
<li><div class="line-block">
<div class="line"><strong>Weak Prior (Small :math:`lambda`)</strong>:</div>
<div class="line">When <span class="math notranslate nohighlight">\(\lambda\)</span> is small, the prior has little influence on the estimated means. In this regime, the data likelihood dominates, and the means are largely determined by the observed data. This can lead to greater flexibility in the estimated means but may also allow for overfitting, especially with noisy data.</div>
</div>
</li>
<li><div class="line-block">
<div class="line"><strong>Strong Prior (Large :math:`lambda`)</strong>:</div>
<div class="line">Conversely, when <span class="math notranslate nohighlight">\(\lambda\)</span> is large, the prior is very informative. In this case, the estimated means are pulled more strongly toward the prior mean <span class="math notranslate nohighlight">\(\mu_0\)</span>. This is particularly useful when you have limited data or when you want to impose prior knowledge about the location of the clusters. However, a very strong prior might oversmooth the model, preventing it from fully adapting to the data.</div>
</div>
</li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[47]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the mean prior distributions as 1D Gaussians</span>
<span class="n">precision_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e3</span><span class="p">,</span> <span class="mf">1e5</span><span class="p">]</span>

<span class="c1"># Create the initial means using K-means on the CPU</span>
<span class="n">init_means_kmeans</span> <span class="o">=</span> <span class="n">GMMInitializer</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">X_tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">k</span><span class="o">=</span><span class="n">n_components</span><span class="p">)</span>
<span class="n">mean_prior</span> <span class="o">=</span> <span class="n">init_means_kmeans</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Create a range for plotting the Gaussian distributions</span>
<span class="n">x_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">precision_values</span><span class="p">),</span> <span class="mi">1</span>
<span class="n">figsize</span> <span class="o">=</span> <span class="n">dynamic_figsize</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">ncols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Mean Prior Distributions (1D Visualization)&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">]</span>
<span class="n">component_labels</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;Component </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">)]</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">precision</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="n">precision_values</span><span class="p">):</span>
    <span class="c1"># Standard deviation of the prior (inverse of precision)</span>
    <span class="n">prior_std</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span>

    <span class="c1"># Plot Gaussian distributions for each component&#39;s mean prior</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">):</span>
        <span class="c1"># Use the first dimension of the mean for visualization</span>
        <span class="n">mean_val</span> <span class="o">=</span> <span class="n">mean_prior</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="c1"># Calculate Gaussian PDF</span>
        <span class="n">gaussian_pdf</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">prior_std</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)))</span> <span class="o">*</span> \
                      <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">x_range</span> <span class="o">-</span> <span class="n">mean_val</span><span class="p">)</span> <span class="o">/</span> <span class="n">prior_std</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">gaussian_pdf</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">component_labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="c1"># Mark the mean with a vertical line</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">mean_val</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Precision λ = </span><span class="si">{</span><span class="n">precision</span><span class="si">}</span><span class="s2"> (σ = </span><span class="si">{</span><span class="n">prior_std</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Mean Value (First Dimension)&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Prior Density&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_priors_11_0.png" src="../_images/notebooks_priors_11_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[48]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a list of precision values to compare</span>
<span class="n">precision_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">,</span> <span class="mf">1e3</span><span class="p">]</span>

<span class="c1"># Create the initial means using K-means on the CPU</span>
<span class="n">init_means_kmeans</span> <span class="o">=</span> <span class="n">GMMInitializer</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">X_tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">k</span><span class="o">=</span><span class="n">n_components</span><span class="p">)</span>
<span class="n">mean_prior</span> <span class="o">=</span> <span class="n">init_means_kmeans</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">precision_values</span><span class="p">),</span> <span class="mi">1</span>
<span class="n">figsize</span> <span class="o">=</span> <span class="n">dynamic_figsize</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">ncols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">precision</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="n">precision_values</span><span class="p">):</span>
    <span class="c1"># Create a new GMM instance for each precision value</span>
    <span class="n">gmm_mean_prior</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span>
        <span class="n">n_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
        <span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span>
        <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">mean_prior</span><span class="o">=</span><span class="n">mean_prior</span><span class="p">,</span>
        <span class="n">mean_precision_prior</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="n">means_init</span><span class="o">=</span><span class="n">mean_prior</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">gmm_mean_prior</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">)</span>

    <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Mean Prior (precision=</span><span class="si">{</span><span class="n">precision</span><span class="si">}</span><span class="s2">)</span><span class="se">\n</span><span class="s2">LL=</span><span class="si">{</span><span class="n">gmm_mean_prior</span><span class="o">.</span><span class="n">lower_bound_</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="c1"># Plot using mode &#39;means&#39; with a base alpha of 0.5</span>
    <span class="n">plot_gmm</span><span class="p">(</span>
        <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
        <span class="n">gmm</span><span class="o">=</span><span class="n">gmm_mean_prior</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span>
        <span class="n">ellipse_alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">ellipse_line_style</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span>
        <span class="n">show_initial_means</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">ellipse_colors</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span>
        <span class="n">ellipse_std_devs</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
        <span class="n">mean_color</span><span class="o">=</span><span class="s1">&#39;yellow&#39;</span><span class="p">,</span>
        <span class="n">mean_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">initial_mean_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">scale_alpha_by_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">scale_size_by_weight</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_priors_12_0.png" src="../_images/notebooks_priors_12_0.png" />
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="The-Gamma-Distribution-and-its-Inverse">
<h2>The Gamma Distribution and its Inverse<a class="headerlink" href="#The-Gamma-Distribution-and-its-Inverse" title="Link to this heading"></a></h2>
<p>The Gamma distribution is given by the density function</p>
<div class="math notranslate nohighlight">
\[f(x; \alpha, \theta) = \frac{1}{\Gamma(\alpha)\,\theta^\alpha}\, x^{\alpha-1} \exp\left(-\frac{x}{\theta}\right), \quad x &gt; 0,\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is the <strong>shape parameter</strong>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta\)</span> is the <strong>scale parameter</strong>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\Gamma(\alpha)\)</span> is the gamma function.</p></li>
</ul>
<p>The mean of the Gamma distribution is</p>
<div class="math notranslate nohighlight">
\[E[X] = \alpha \theta.\]</div>
<p>If we define</p>
<div class="math notranslate nohighlight">
\[Y = \frac{1}{X},\]</div>
<p>then using the standard transformation method the density of <span class="math notranslate nohighlight">\(Y\)</span> becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
f_Y(y) &amp;= f_X\left(\frac{1}{y}\right) \left|\frac{d}{dy}\left(\frac{1}{y}\right)\right| \\
&amp;= \frac{1}{\Gamma(\alpha)\,\theta^\alpha} \left(\frac{1}{y}\right)^{\alpha-1} \exp\left(-\frac{1}{\theta y}\right) \frac{1}{y^2} \\
&amp;= \frac{1}{\Gamma(\alpha)\,\theta^\alpha}\, y^{-\alpha-1} \exp\left(-\frac{1}{\theta y}\right), \quad y &gt; 0.
\end{aligned}\end{split}\]</div>
<p>This density is of the form of an inverse Gamma distribution. A standard parameterization for the inverse Gamma distribution is:</p>
<div class="math notranslate nohighlight">
\[f_Y(y; \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}\, y^{-\alpha-1} \exp\left(-\frac{\beta}{y}\right), \quad y &gt; 0,\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta &gt; 0\)</span> is the <strong>scale parameter</strong> of the inverse Gamma. By comparing the two forms, we identify</p>
<div class="math notranslate nohighlight">
\[\beta^\alpha = \frac{1}{\theta^\alpha} \quad \Longrightarrow \quad \beta = \frac{1}{\theta}.\]</div>
<section id="Mean-of-the-Inverse-Gamma-Distribution">
<h3>Mean of the Inverse Gamma Distribution<a class="headerlink" href="#Mean-of-the-Inverse-Gamma-Distribution" title="Link to this heading"></a></h3>
<p>For the inverse Gamma distribution with parameters <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>, the mean exists for <span class="math notranslate nohighlight">\(\alpha &gt; 1\)</span> and is given by</p>
<div class="math notranslate nohighlight">
\[E[Y] = \frac{\beta}{\alpha - 1}.\]</div>
<p>Substituting <span class="math notranslate nohighlight">\(\beta = \frac{1}{\theta}\)</span> into the formula, we get</p>
<div class="math notranslate nohighlight">
\[E[Y] = \frac{1/\theta}{\alpha - 1} = \frac{1}{\theta (\alpha - 1)}.\]</div>
</section>
<section id="Impact-of-the-Parameters">
<h3>Impact of the Parameters<a class="headerlink" href="#Impact-of-the-Parameters" title="Link to this heading"></a></h3>
<ul>
<li><div class="line-block">
<div class="line"><strong>Shape (:math:`alpha`)</strong>:</div>
<div class="line">The shape parameter <span class="math notranslate nohighlight">\(\alpha\)</span> affects the tail behavior and the concentration of the distribution. Increasing <span class="math notranslate nohighlight">\(\alpha\)</span> (with <span class="math notranslate nohighlight">\(\beta\)</span> fixed) causes both the mean and the mode to decrease:</div>
</div>
<div class="math notranslate nohighlight">
\[E[Y] = \frac{\beta}{\alpha - 1}, \qquad \text{mode} = \frac{\beta}{\alpha + 1}.\]</div>
<p>Thus, a higher <span class="math notranslate nohighlight">\(\alpha\)</span> results in a distribution that is more concentrated near zero and has lighter tails, while a lower <span class="math notranslate nohighlight">\(\alpha\)</span> produces heavier tails.</p>
</li>
<li><div class="line-block">
<div class="line"><strong>Scale (:math:`beta`)</strong>:</div>
<div class="line">The scale parameter <span class="math notranslate nohighlight">\(\beta\)</span> shifts the distribution. A larger <span class="math notranslate nohighlight">\(\beta\)</span> leads to higher values of the mean and mode. In many Bayesian contexts, the inverse gamma is obtained by inverting a Gamma-distributed variable with scale parameter <span class="math notranslate nohighlight">\(\theta\)</span>, where <span class="math notranslate nohighlight">\(\beta = \frac{1}{\theta}\)</span>. In that case, increasing <span class="math notranslate nohighlight">\(\theta\)</span> (making the Gamma distribution more spread out) results in a smaller <span class="math notranslate nohighlight">\(\beta\)</span>, which in turn shifts the inverse gamma distribution toward lower
values.</div>
</div>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="Visualizing-the-Inverse-Gamma-Distribution">
<h2>Visualizing the Inverse Gamma Distribution<a class="headerlink" href="#Visualizing-the-Inverse-Gamma-Distribution" title="Link to this heading"></a></h2>
<p>The three panels below demonstrate the following scenarios:</p>
<ol class="arabic">
<li><div class="line-block">
<div class="line"><strong>Varying :math:`alpha` with Fixed :math:`theta`:</strong></div>
<div class="line">How changes in the shape parameter alter the spread and concentration of the distribution.</div>
</div>
</li>
<li><div class="line-block">
<div class="line"><strong>Fixed :math:`alpha` with Varying :math:`theta`:</strong></div>
<div class="line">The effect of the scale parameter on the location and spread of the distribution.</div>
</div>
</li>
<li><div class="line-block">
<div class="line"><strong>Varying Both to Maintain the Same Mode:</strong></div>
<div class="line">How different combinations of <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span> can yield the same mode, illustrating the trade-off between the parameters.</div>
</div>
</li>
</ol>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[49]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">18</span><span class="p">))</span>

<span class="c1"># -------------------------------</span>
<span class="c1"># Plot 1: Varying $\alpha$, fixed $\theta$</span>
<span class="c1"># -------------------------------</span>
<span class="n">fixed_theta</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>

<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">fixed_theta</span>
    <span class="n">pdf_vals</span> <span class="o">=</span> <span class="n">invgamma</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">/</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pdf_vals</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2">,</span><span class="se">\\</span><span class="s2">,</span><span class="se">\\</span><span class="s2">theta=</span><span class="si">{</span><span class="n">fixed_theta</span><span class="si">}</span><span class="s2">,</span><span class="se">\\</span><span class="s2">,</span><span class="se">\\</span><span class="s2">mathrm</span><span class="se">{{</span><span class="s2">mode</span><span class="se">}}</span><span class="s2">=</span><span class="si">{</span><span class="n">mode</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">$&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">line</span><span class="o">.</span><span class="n">get_color</span><span class="p">(),</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Inverse Gamma PDF (Varying $</span><span class="se">\\</span><span class="s2">alpha$, Fixed $</span><span class="se">\\</span><span class="s2">theta$)&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Density&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># -------------------------------</span>
<span class="c1"># Plot 2: Fixed $\alpha$, varying $\theta$</span>
<span class="c1"># -------------------------------</span>
<span class="n">fixed_alpha</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>

<span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">:</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">theta</span>
    <span class="n">pdf_vals</span> <span class="o">=</span> <span class="n">invgamma</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">fixed_alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">/</span> <span class="p">(</span><span class="n">fixed_alpha</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pdf_vals</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">alpha=</span><span class="si">{</span><span class="n">fixed_alpha</span><span class="si">}</span><span class="s2">,</span><span class="se">\\</span><span class="s2">,</span><span class="se">\\</span><span class="s2">theta=</span><span class="si">{</span><span class="n">theta</span><span class="si">}</span><span class="s2">,</span><span class="se">\\</span><span class="s2">,</span><span class="se">\\</span><span class="s2">mathrm</span><span class="se">{{</span><span class="s2">mode</span><span class="se">}}</span><span class="s2">=</span><span class="si">{</span><span class="n">mode</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">$&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">line</span><span class="o">.</span><span class="n">get_color</span><span class="p">(),</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Inverse Gamma PDF (Fixed $</span><span class="se">\\</span><span class="s2">alpha$, Varying $</span><span class="se">\\</span><span class="s2">theta$)&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Density&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># -------------------------------</span>
<span class="c1"># Plot 3: Varying both $\alpha$ and $\theta$ to get the same mode</span>
<span class="c1"># -------------------------------</span>
<span class="n">alphas_equal</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>
<span class="n">thetas_equal</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.125</span><span class="p">]</span>

<span class="k">for</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">theta</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">alphas_equal</span><span class="p">,</span> <span class="n">thetas_equal</span><span class="p">):</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">theta</span>
    <span class="n">pdf_vals</span> <span class="o">=</span> <span class="n">invgamma</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">/</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pdf_vals</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2">,</span><span class="se">\\</span><span class="s2">,</span><span class="se">\\</span><span class="s2">theta=</span><span class="si">{</span><span class="n">theta</span><span class="si">}</span><span class="s2">,</span><span class="se">\\</span><span class="s2">,</span><span class="se">\\</span><span class="s2">mathrm</span><span class="se">{{</span><span class="s2">mode</span><span class="se">}}</span><span class="s2">=</span><span class="si">{</span><span class="n">mode</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">$&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">line</span><span class="o">.</span><span class="n">get_color</span><span class="p">(),</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Inverse Gamma PDF (Varying both $</span><span class="se">\\</span><span class="s2">alpha$ and $</span><span class="se">\\</span><span class="s2">theta$ to get the same mode)&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Density&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_priors_15_0.png" src="../_images/notebooks_priors_15_0.png" />
</div>
</div>
</section>
<hr class="docutils" />
<section id="The-Wishart-Distribution-and-its-Inverse">
<h2>The Wishart Distribution and its Inverse<a class="headerlink" href="#The-Wishart-Distribution-and-its-Inverse" title="Link to this heading"></a></h2>
<p>In Bayesian modeling of Gaussian mixtures, a key step is placing a prior on the covariance matrices of each Gaussian component. Since these matrices must be positive-definite (i.e., all eigenvalues are positive), the prior must respect this constraint. While the (non-inverse) Wishart distribution is a natural multivariate analogue of the Gamma distribution, it has characteristics that are not ideal for covariance priors in a MAP estimation context. Instead, the <strong>inverse Wishart</strong> (and its
one-dimensional counterpart, the <strong>inverse Gamma</strong>) is commonly used. Let’s explore why.</p>
<section id="From-the-Gamma-to-the-Wishart-Distribution">
<h3>From the Gamma to the Wishart Distribution<a class="headerlink" href="#From-the-Gamma-to-the-Wishart-Distribution" title="Link to this heading"></a></h3>
<section id="The-Gamma-Distribution-(Scalar-Case)">
<h4>The Gamma Distribution (Scalar Case)<a class="headerlink" href="#The-Gamma-Distribution-(Scalar-Case)" title="Link to this heading"></a></h4>
<p>The Gamma distribution is defined as</p>
<div class="math notranslate nohighlight">
\[f(x; \alpha, \theta) = \frac{1}{\Gamma(\alpha)\,\theta^\alpha}\, x^{\alpha-1} \exp\left(-\frac{x}{\theta}\right), \quad x &gt; 0,\]</div>
<p>where:</p>
<ul class="simple">
<li><p>$ :nbsphinx-math:<a href="#id1"><span class="problematic" id="id2">`</span></a>alpha <a href="#id3"><span class="problematic" id="id4">`</span></a>$ is the <strong>shape parameter</strong>,</p></li>
<li><p>$ :nbsphinx-math:<a href="#id5"><span class="problematic" id="id6">`</span></a>theta <a href="#id7"><span class="problematic" id="id8">`</span></a>$ is the <strong>scale parameter</strong>, and</p></li>
<li><p>$ <span class="math">\Gamma`(:nbsphinx-math:</span>alpha`) $ is the Gamma function.</p></li>
</ul>
<p>For many Bayesian models, the Gamma distribution is used as a conjugate prior for positive parameters (e.g. precision). Notice that—even though certain parameter choices can put high density near zero—its support is strictly $ (0, <span class="math">\infty</span>) $, so the probability of $ x=0 $ is exactly zero.</p>
</section>
<section id="The-Wishart-Distribution-(Multivariate-Case)">
<h4>The Wishart Distribution (Multivariate Case)<a class="headerlink" href="#The-Wishart-Distribution-(Multivariate-Case)" title="Link to this heading"></a></h4>
<p>When we generalize to matrices, the Wishart distribution arises naturally. Its density over $ d :nbsphinx-math:<a href="#id9"><span class="problematic" id="id10">`</span></a>times <a href="#id11"><span class="problematic" id="id12">`</span></a>d $ positive-definite matrices is</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{S}; \nu, \Psi) \propto |\mathbf{S}|^{\frac{\nu-d-1}{2}} \exp\left(-\frac{1}{2}\operatorname{tr}\left(\Psi^{-1} \mathbf{S}\right)\right),\]</div>
<p>where:</p>
<ul class="simple">
<li><p>$ <span class="math">\mathbf{S}</span> $ is a positive-definite matrix,</p></li>
<li><p>$ :nbsphinx-math:<a href="#id13"><span class="problematic" id="id14">`</span></a>nu <a href="#id15"><span class="problematic" id="id16">`</span></a>$ is the <strong>degrees of freedom</strong> (playing a role similar to the shape parameter),</p></li>
<li><p>$ :nbsphinx-math:<a href="#id17"><span class="problematic" id="id18">`</span></a>Psi <a href="#id19"><span class="problematic" id="id20">`</span></a>$ is the <strong>scale matrix</strong>, and</p></li>
<li><p>$ d $ is the dimension.</p></li>
</ul>
<p>For $ d = 1 $, the Wishart reduces to the Gamma distribution (up to constants), which shows their close connection.</p>
</section>
</section>
<section id="Why-Use-the-Inverse-Wishart-and-Inverse-Gamma?">
<h3>Why Use the Inverse Wishart and Inverse Gamma?<a class="headerlink" href="#Why-Use-the-Inverse-Wishart-and-Inverse-Gamma?" title="Link to this heading"></a></h3>
<p>There are two main reasons for preferring the inverse distributions as priors on covariance matrices in a GMM:</p>
<p><strong>Zero Probability at Zero</strong></p>
<p>Even though a standard Gamma (or Wishart) distribution can be parameterized to place a lot of mass near zero, its density is not zero at $ x=0 $ (or at a singular covariance matrix). This is problematic for covariance matrices because a zero covariance (or a singular matrix) is not a valid parameter for a Gaussian distribution.</p>
<p>In contrast, the inverse Gamma and inverse Wishart distributions have <strong>support strictly on $ (0, :nbsphinx-math:`infty`) $ or on the set of non-singular positive-definite matrices</strong>, meaning that the probability of a covariance parameter being exactly zero is exactly zero. This ensures that our prior never “allows” a zero (or degenerate) covariance, which is crucial for well-defined Gaussian likelihoods.</p>
<p><strong>Longer Right-Hand Tail</strong></p>
<p>The inverse distributions typically have a heavier (longer) tail on the right-hand side. This longer tail means that while the prior penalizes extremely small covariances (preventing singularity), it is more tolerant of large covariance estimates.</p>
<p>For covariance matrices, this is important because the empirical covariance can vary widely—especially in high-dimensional settings. The long tail ensures that the prior does not overly constrain large values, thus preserving flexibility. In 2D, for example, the determinant of the covariance matrix (which is proportional to the area of the corresponding ellipse) can be thought of as capturing the “size” of the uncertainty. A heavy right tail in the inverse Wishart better accommodates the
possibility of large uncertainty, similar in spirit (though not strictly proportional) to how the surface area of a circle increases with its radius.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[50]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initial covariance matrix</span>
<span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="c1"># Degrees of freedom and scale factors</span>
<span class="n">dfs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">scales</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">scale_matrices</span> <span class="o">=</span> <span class="p">[</span><span class="n">covariance_matrix</span> <span class="o">*</span> <span class="n">scale</span> <span class="k">for</span> <span class="n">scale</span> <span class="ow">in</span> <span class="n">scales</span><span class="p">]</span>

<span class="c1"># Collect inverse Wishart samples</span>
<span class="n">inv_wishart_samples_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">dfs</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">scale_matrix</span> <span class="ow">in</span> <span class="n">scale_matrices</span><span class="p">:</span>
        <span class="c1"># Sample from the Wishart distribution</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">wishart</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale_matrix</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000000</span><span class="p">)</span>
        <span class="c1"># Invert the samples to obtain inverse Wishart samples</span>
        <span class="n">inv_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
        <span class="n">inv_wishart_samples_list</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">df</span><span class="p">,</span> <span class="n">scale_matrix</span><span class="p">,</span> <span class="n">inv_samples</span><span class="p">))</span>

<span class="c1"># Set fixed axis limits (adjust as needed)</span>
<span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">2</span>

<span class="c1"># Set up the figure for multiple hexbin plots</span>
<span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dfs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale_matrices</span><span class="p">)</span>
<span class="n">figsize</span> <span class="o">=</span> <span class="n">dynamic_figsize</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">,</span> <span class="n">base_width</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">base_height</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">ncols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Hexbin Plots with Smoothed Gradient Contours</span><span class="se">\n</span><span class="s2">of 2D Inverse Wishart Distributions&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="c1"># Loop over different degrees of freedom and scale matrices</span>
<span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">df</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dfs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">scale_matrix</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">scale_matrices</span><span class="p">):</span>
        <span class="n">df_current</span><span class="p">,</span> <span class="n">scale_matrix_current</span><span class="p">,</span> <span class="n">inv_samples</span> <span class="o">=</span> <span class="n">inv_wishart_samples_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Extract diagonal elements (variances) from the inverse Wishart samples</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inv_samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">inv_samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

        <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
        <span class="c1"># Create hexbin plot</span>
        <span class="n">hb</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">hexbin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">gridsize</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;PiYG&#39;</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">[</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">,</span> <span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span><span class="p">])</span>
        <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">hb</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s1">&#39;vertical&#39;</span><span class="p">)</span>

        <span class="c1"># Compute a 2D histogram for contour lines</span>
        <span class="n">H</span><span class="p">,</span> <span class="n">xedges</span><span class="p">,</span> <span class="n">yedges</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="p">[[</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">],</span> <span class="p">[</span><span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span><span class="p">]])</span>
        <span class="c1"># Smooth the histogram using a Gaussian filter</span>
        <span class="n">H_smooth</span> <span class="o">=</span> <span class="n">gaussian_filter</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Compute the centers of the bins</span>
        <span class="n">Xc</span> <span class="o">=</span> <span class="p">(</span><span class="n">xedges</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">xedges</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">Yc</span> <span class="o">=</span> <span class="p">(</span><span class="n">yedges</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">yedges</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">X_mesh</span><span class="p">,</span> <span class="n">Y_mesh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">Xc</span><span class="p">,</span> <span class="n">Yc</span><span class="p">)</span>

        <span class="c1"># Overlay smoothed contour lines to show density gradients</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X_mesh</span><span class="p">,</span> <span class="n">Y_mesh</span><span class="p">,</span> <span class="n">H_smooth</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span><span class="p">])</span>

        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DF=</span><span class="si">{</span><span class="n">df</span><span class="si">}</span><span class="s2">, Scale=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">scale_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Diagonal Element 1&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Diagonal Element 2&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">rect</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.98</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_priors_17_0.png" src="../_images/notebooks_priors_17_0.png" />
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="Covariance-Priors-in-GMM">
<h2>Covariance Priors in GMM<a class="headerlink" href="#Covariance-Priors-in-GMM" title="Link to this heading"></a></h2>
<p>Next, we demonstrate how covariance priors affect the GMM. In the following two sections, we vary:</p>
<ol class="arabic simple">
<li><p><strong>Prior Strength:</strong> The covariance prior is scaled by different factors.</p></li>
<li><p><strong>Degrees of Freedom (DOF):</strong> The DOF of the prior distribution is varied.</p></li>
<li><p><strong>Strength and DOF:</strong> Varying both the prior strength and the DOF.</p></li>
</ol>
<section id="1.-Varying-Prior-Strength">
<h3>1. Varying Prior Strength<a class="headerlink" href="#1.-Varying-Prior-Strength" title="Link to this heading"></a></h3>
<p>Here, the DOF is fixed and the covariance prior strength is varied.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[56]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_covariance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># shape (2, 2)</span>
<span class="n">data_covariance</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data_covariance</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">data_covariance</span> <span class="o">=</span> <span class="n">data_covariance</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">degrees_of_freedom_prior</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">n_features</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">prior_strengths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">25</span><span class="p">]</span>

<span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prior_strengths</span><span class="p">),</span> <span class="mi">1</span>
<span class="n">figsize</span> <span class="o">=</span> <span class="n">dynamic_figsize</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">ncols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Covariance Priors: Varying Strength&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">strength</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="n">prior_strengths</span><span class="p">):</span>
    <span class="n">cov_prior</span> <span class="o">=</span> <span class="n">data_covariance</span> <span class="o">*</span> <span class="n">strength</span>

    <span class="n">dummy_mean_prior</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">dummy_mean_precision_prior</span> <span class="o">=</span> <span class="mf">1e-10</span>  <span class="c1"># effectively no strong push on means</span>

    <span class="n">gmm_cov_prior</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span>
        <span class="n">n_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
        <span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span>
        <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="c1"># Covariance prior:</span>
        <span class="n">covariance_prior</span><span class="o">=</span><span class="n">cov_prior</span><span class="p">,</span>
        <span class="n">degrees_of_freedom_prior</span><span class="o">=</span><span class="n">degrees_of_freedom_prior</span><span class="p">,</span>
        <span class="c1"># Provide &quot;dummy&quot; mean priors so code doesn&#39;t crash</span>
        <span class="n">mean_prior</span><span class="o">=</span><span class="n">dummy_mean_prior</span><span class="p">,</span>
        <span class="n">mean_precision_prior</span><span class="o">=</span><span class="n">dummy_mean_precision_prior</span>
    <span class="p">)</span>
    <span class="n">gmm_cov_prior</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">)</span>

    <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;DOF=</span><span class="si">{</span><span class="n">degrees_of_freedom_prior</span><span class="si">}</span><span class="s2">, Strength=</span><span class="si">{</span><span class="n">strength</span><span class="si">}</span><span class="se">\n</span><span class="s2">LL=</span><span class="si">{</span><span class="n">gmm_cov_prior</span><span class="o">.</span><span class="n">lower_bound_</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="c1"># Use mode &#39;covariances&#39; with the sequential colormap (default: Greens) and std_devs [1,2,3]</span>
    <span class="n">plot_gmm</span><span class="p">(</span>
        <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
        <span class="n">gmm</span><span class="o">=</span><span class="n">gmm_cov_prior</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span>
        <span class="n">ellipse_colors</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span>
        <span class="n">ellipse_std_devs</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">rect</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.98</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_priors_19_0.png" src="../_images/notebooks_priors_19_0.png" />
</div>
</div>
</section>
<section id="2.-Varying-Degrees-of-Freedom-(DOF)">
<h3>2. Varying Degrees of Freedom (DOF)<a class="headerlink" href="#2.-Varying-Degrees-of-Freedom-(DOF)" title="Link to this heading"></a></h3>
<p>Here, the covariance prior strength is fixed and we vary the DOF.</p>
<p>Note: For 2D, DOF must be greater than 1.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[58]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_covariance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># shape (2, 2)</span>
<span class="n">data_covariance</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data_covariance</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">data_covariance</span> <span class="o">=</span> <span class="n">data_covariance</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Instead of varying strength, we fix the strength and vary DOF.</span>
<span class="n">dof_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_features</span> <span class="o">+</span> <span class="mi">8</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="mi">98</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="mi">498</span><span class="p">]</span>
<span class="n">fixed_strength</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dof_values</span><span class="p">),</span> <span class="mi">1</span>
<span class="n">figsize</span> <span class="o">=</span> <span class="n">dynamic_figsize</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">ncols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Covariance Priors: Varying DOF&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">dof</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="n">dof_values</span><span class="p">):</span>
    <span class="n">cov_prior</span> <span class="o">=</span> <span class="n">data_covariance</span> <span class="o">*</span> <span class="n">fixed_strength</span>

    <span class="n">dummy_mean_prior</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">dummy_mean_precision_prior</span> <span class="o">=</span> <span class="mf">1e-10</span>  <span class="c1"># negligible influence on means</span>

    <span class="n">gmm_cov_prior</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span>
        <span class="n">n_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
        <span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span>
        <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">covariance_prior</span><span class="o">=</span><span class="n">cov_prior</span><span class="p">,</span>
        <span class="n">degrees_of_freedom_prior</span><span class="o">=</span><span class="n">dof</span><span class="p">,</span>
        <span class="n">mean_prior</span><span class="o">=</span><span class="n">dummy_mean_prior</span><span class="p">,</span>
        <span class="n">mean_precision_prior</span><span class="o">=</span><span class="n">dummy_mean_precision_prior</span>
    <span class="p">)</span>
    <span class="n">gmm_cov_prior</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">)</span>

    <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;DOF=</span><span class="si">{</span><span class="n">dof</span><span class="si">}</span><span class="s2">, Strength=</span><span class="si">{</span><span class="n">fixed_strength</span><span class="si">}</span><span class="se">\n</span><span class="s2">LL=</span><span class="si">{</span><span class="n">gmm_cov_prior</span><span class="o">.</span><span class="n">lower_bound_</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="c1"># Use mode &#39;covariances&#39; with the sequential colormap (default: Greens) and std_devs [1, 2, 3].</span>
    <span class="n">plot_gmm</span><span class="p">(</span>
        <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
        <span class="n">gmm</span><span class="o">=</span><span class="n">gmm_cov_prior</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span>
        <span class="n">ellipse_colors</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>
        <span class="n">ellipse_std_devs</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
    <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">rect</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.98</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_priors_21_0.png" src="../_images/notebooks_priors_21_0.png" />
</div>
</div>
</section>
<section id="3.-Varying-Degrees-of-Freedom-(DOF)">
<h3>3. Varying Degrees of Freedom (DOF)<a class="headerlink" href="#3.-Varying-Degrees-of-Freedom-(DOF)" title="Link to this heading"></a></h3>
<p>Here, both the covariance prior strength and the DOF are varied.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[60]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute data covariance from the tensor data.</span>
<span class="n">data_covariance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># shape (2, 2)</span>
<span class="n">data_covariance</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data_covariance</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">data_covariance</span> <span class="o">=</span> <span class="n">data_covariance</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Define the lists of prior strengths and degrees of freedom to explore.</span>
<span class="n">prior_strengths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">dof_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_features</span> <span class="o">+</span> <span class="mi">8</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="mi">98</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="mi">498</span><span class="p">]</span>

<span class="c1"># Create a 3x3 grid.</span>
<span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dof_values</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">prior_strengths</span><span class="p">)</span>
<span class="n">figsize</span> <span class="o">=</span> <span class="n">dynamic_figsize</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">,</span> <span class="n">base_width</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">base_height</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">ncols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Covariance Priors: Varying Strength and DOF&quot;</span><span class="p">)</span>

<span class="c1"># Loop over DOF values (rows) and prior strengths (columns)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dof</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dof_values</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">strength</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">prior_strengths</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
        <span class="c1"># Compute covariance prior for this combination.</span>
        <span class="n">cov_prior</span> <span class="o">=</span> <span class="n">data_covariance</span> <span class="o">*</span> <span class="n">strength</span>

        <span class="n">dummy_mean_prior</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">dummy_mean_precision_prior</span> <span class="o">=</span> <span class="mf">1e-10</span>  <span class="c1"># negligible influence on means</span>

        <span class="n">gmm_cov_prior</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span>
            <span class="n">n_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
            <span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span>
            <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">covariance_prior</span><span class="o">=</span><span class="n">cov_prior</span><span class="p">,</span>
            <span class="n">degrees_of_freedom_prior</span><span class="o">=</span><span class="n">dof</span><span class="p">,</span>
            <span class="n">mean_prior</span><span class="o">=</span><span class="n">dummy_mean_prior</span><span class="p">,</span>
            <span class="n">mean_precision_prior</span><span class="o">=</span><span class="n">dummy_mean_precision_prior</span>
        <span class="p">)</span>
        <span class="n">gmm_cov_prior</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">)</span>

        <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;DOF=</span><span class="si">{</span><span class="n">dof</span><span class="si">}</span><span class="s2">, Strength=</span><span class="si">{</span><span class="n">strength</span><span class="si">}</span><span class="se">\n</span><span class="s2">LL=</span><span class="si">{</span><span class="n">gmm_cov_prior</span><span class="o">.</span><span class="n">lower_bound_</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="c1"># Plot using mode &#39;covariances&#39;. This mode uses a sequential colormap (here &#39;Greens&#39;)</span>
        <span class="c1"># to assign colors to the ellipses so that the inner (1-std) ellipse is the darkest.</span>
        <span class="n">plot_gmm</span><span class="p">(</span>
            <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
            <span class="n">gmm</span><span class="o">=</span><span class="n">gmm_cov_prior</span><span class="p">,</span>
            <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
            <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span>
            <span class="n">ellipse_colors</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span>
            <span class="n">ellipse_std_devs</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="c1"># Add legend (if any labels are present; otherwise this will issue a warning)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">rect</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.98</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_priors_23_0.png" src="../_images/notebooks_priors_23_0.png" />
</div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="metrics.html" class="btn btn-neutral float-left" title="Clustering Metrics: Comprehensive Evaluation of Gaussian Mixture Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="visualise.html" class="btn btn-neutral float-right" title="TorchGMM Plotting Function Comprehensive Demo" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Adrián A. Sousa-Poza.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>