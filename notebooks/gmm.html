

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tutorial: A Gaussian Mixture Model &mdash; TorchGMM 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=01f34227"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Metrics" href="metrics.html" />
    <link rel="prev" title="Tutorials" href="../source/tutorials.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            TorchGMM
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../source/modules.html">Modules</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../source/tutorials.html">Tutorials</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Tutorial: A Gaussian Mixture Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics.html">Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="priors.html">Priors</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">TorchGMM</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../source/tutorials.html">Tutorials</a></li>
      <li class="breadcrumb-item active">Tutorial: A Gaussian Mixture Model</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/notebooks/gmm.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Tutorial:-A-Gaussian-Mixture-Model">
<h1>Tutorial: A Gaussian Mixture Model<a class="headerlink" href="#Tutorial:-A-Gaussian-Mixture-Model" title="Link to this heading"></a></h1>
<p>In this tutorial, we demonstrate how to use the <code class="docutils literal notranslate"><span class="pre">GaussianMixture</span></code> class:</p>
<ul class="simple">
<li><p>Fit the GMM with various covariance types (full, diag, spherical, tied).</p></li>
<li><p>Explore different initialization methods (random, points, kpp, kmeans, maxdist).</p></li>
<li><p>Visualize model results, including responsibilities and generated samples.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import torch
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
from matplotlib.colors import ListedColormap
import importlib
import os
import sys
sys.path.append(&#39;../../..&#39;)
#print(os.listdir(&quot;../../..&quot;))

import utils.gmm
import utils.metrics
importlib.reload(utils.gmm)
importlib.reload(utils.metrics)
from utils.gmm import GaussianMixture

device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
random_state = 0
np.random.seed(random_state)
torch.manual_seed(random_state)
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;torch._C.Generator at 0x7f4f1c0732b0&gt;
</pre></div></div>
</div>
<p>Let’s generate four clusters in 2D with different covariances.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n_samples = [1000, 1000, 1000, 1000]
centers = [np.array([0, 0]),
           np.array([0, 2]),
           np.array([2, 0]),
           np.array([2, 2])]
covs = [
    np.array([[0.0, -0.5], [1.5, 0.5]]),  # full covariance example
    0.7 * np.eye(2),                      # spherical-like
    0.5 * np.eye(2),                      # smaller spherical-like
    np.array([[0.0, 0.2], [0.4, 1.7]])    # full covariance example
]

components = []
for n, center, cov in zip(n_samples, centers, covs):
    # Generate multivariate normal data
    samples = np.dot(np.random.randn(n, 2), cov) + center
    components.append(samples)

X = np.vstack(components)  # shape (4000, 2)
X_tensor = torch.tensor(X, dtype=torch.float32, device=device)

n_features = X.shape[1]
n_components = len(n_samples)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def plot_gmm_results(
    gmm,
    X,
    labels,
    ax=None,
    title=&#39;GMM Results&#39;,
    init_means=None
):
    &quot;&quot;&quot;
    Plot data points colored by cluster, the final GMM ellipses,
    and optionally the initial means in red.

    Parameters
    ----------
    gmm : GaussianMixture
        A fitted GaussianMixture instance.
    X : np.ndarray
        Original 2D data (shape: (N, 2)).
    labels : np.ndarray
        Predicted cluster labels for each point in X.
    ax : matplotlib.axes.Axes
        Axes on which to plot. If None, uses current Axes.
    title : str
        Title for the subplot.
    init_means : torch.Tensor or None
        Initial means (k, 2) if you want to display them in red &#39;+&#39;.
    &quot;&quot;&quot;
    if ax is None:
        ax = plt.gca()

    # Distinct colors for each cluster
    cmap = ListedColormap(plt.cm.tab10(np.linspace(0, 1, gmm.n_components)))

    # Plot data points by predicted labels
    for i, color in zip(range(gmm.n_components), cmap.colors):
        mask = (labels == i)
        ax.scatter(X[mask, 0], X[mask, 1], c=[color], s=2, label=f&#39;Cluster {i}&#39;, alpha=0.6)

    # Plot final means &amp; covariances
    for n, color in zip(range(gmm.n_components), cmap.colors):
        mean = gmm.means_[n].detach().cpu().numpy()

        # Determine the covariance based on gmm.covariance_type
        if gmm.covariance_type == &#39;full&#39;:
            cov = gmm.covariances_[n].detach().cpu().numpy()
        elif gmm.covariance_type == &#39;diag&#39;:
            diag_vals = gmm.covariances_[n].detach().cpu().numpy()
            cov = np.diag(diag_vals)
        elif gmm.covariance_type == &#39;spherical&#39;:
            var = gmm.covariances_[n].detach().cpu().item()  # scalar
            cov = np.eye(gmm.n_features) * var
        elif gmm.covariance_type == &#39;tied_full&#39;:
            cov = gmm.covariances_.detach().cpu().numpy()  # single shared matrix
        elif gmm.covariance_type == &#39;tied_diag&#39;:
            diag_vals = gmm.covariances_.detach().cpu().numpy()
            cov = np.diag(diag_vals)
        elif gmm.covariance_type == &#39;tied_spherical&#39;:
            var = gmm.covariances_.detach().cpu().item()
            cov = np.eye(gmm.n_features) * var
        else:
            raise ValueError(f&quot;Unsupported covariance_type: {gmm.covariance_type}&quot;)

        # Decompose covariance to plot an ellipse
        vals, vecs = np.linalg.eigh(cov)
        order = vals.argsort()[::-1]
        vals, vecs = vals[order], vecs[:, order]
        angle = np.degrees(np.arctan2(*vecs[:, 0][::-1]))

        # Plot ellipses for 1,2,3 std devs
        for std_dev, alpha_ellipse in zip([1, 2, 3], [0.3, 0.2, 0.1]):
            width, height = 2 * np.sqrt(vals) * std_dev
            e = Ellipse(
                mean,
                width=width,
                height=height,
                angle=angle,
                facecolor=color,
                alpha=alpha_ellipse,
                edgecolor=None
            )
            ax.add_patch(e)

        # Mark final mean as black circle
        ax.plot(mean[0], mean[1], &#39;ko&#39;, markersize=4)

    # Optionally plot the initial means in red
    if init_means is not None:
        init_means_cpu = init_means.detach().cpu().numpy()
        for i in range(gmm.n_components):
            ax.plot(init_means_cpu[i, 0], init_means_cpu[i, 1],
                    &#39;r+&#39;, markersize=10, markeredgewidth=2)

    ax.set_title(title)
    ax.axis(&#39;equal&#39;)
<br/></pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Initialize the GMM
gmm = GaussianMixture(
    n_features=n_features,
    n_components=n_components,
    covariance_type=&#39;full&#39;,
    tol=1e-4,
    reg_covar=1e-6,
    max_iter=1000,
    init_params=&#39;random&#39;,
    cov_init_method=&#39;eye&#39;,
    weights_init=None,
    means_init=None,
    covariances_init=None,
    n_init = 5,
    random_state=None,
    warm_start=False,
    verbose=True,
    verbose_interval=10,
    device=&#39;cpu&#39;,
)
# Fit the GMM
gmm.fit(X_tensor)

# Get predictions
y_pred = gmm.predict(X_tensor)

# Compute per-sample log-likelihoods
log_probs = gmm.score_samples(X_tensor)

# Get probabilities for each sample for each component
probs = gmm.predict_proba(X_tensor)
probs = probs.detach()

# Generate new samples
n_samples_to_generate = 4000
gmm_samples, gmm_labels = gmm.sample(n_samples_to_generate)
gmm_samples = gmm_samples.detach().cpu().numpy()
gmm_labels = gmm_labels.detach().cpu().numpy()

# Compute probabilities for each generated sample
generated_probs = gmm.predict_proba(torch.tensor(gmm_samples, dtype=torch.float32).to(device))
generated_probs = generated_probs.detach().cpu().numpy()

print(&#39;Mean per-sample log-likelihood:&#39;, gmm.score(X_tensor))
print(&#39;Std per-sample log-likelihood:&#39;, torch.std(log_probs).item())
print(&#39;Lower bound:&#39;, gmm.lower_bound_)
print(&#39;Number of iterations:&#39;, gmm.n_iter_)
print(&#39;Sum of log-likelihoods:&#39;, torch.sum(log_probs).item())
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[InitRun 0] Iter 0, lower bound: -5.38992
[InitRun 0] Iter 10, lower bound: -3.21788
[InitRun 0] Iter 20, lower bound: -3.18038
[InitRun 0] Iter 30, lower bound: -2.77929
[InitRun 0] Converged at iteration 38, lower bound=-2.73262
[InitRun 1] Iter 0, lower bound: -3.91078
[InitRun 1] Iter 10, lower bound: -3.17127
[InitRun 1] Iter 20, lower bound: -3.12612
[InitRun 1] Iter 30, lower bound: -2.74164
[InitRun 1] Converged at iteration 38, lower bound=-2.73285
[InitRun 2] Iter 0, lower bound: -4.41680
[InitRun 2] Iter 10, lower bound: -3.22166
[InitRun 2] Iter 20, lower bound: -3.20861
[InitRun 2] Iter 30, lower bound: -3.20091
[InitRun 2] Iter 40, lower bound: -2.74187
[InitRun 2] Converged at iteration 50, lower bound=-2.73273
[InitRun 3] Iter 0, lower bound: -4.80912
[InitRun 3] Iter 10, lower bound: -3.16844
[InitRun 3] Iter 20, lower bound: -3.06464
[InitRun 3] Iter 30, lower bound: -2.73293
[InitRun 3] Converged at iteration 31, lower bound=-2.73272
[InitRun 4] Iter 0, lower bound: -4.88786
[InitRun 4] Iter 10, lower bound: -3.23513
[InitRun 4] Iter 20, lower bound: -3.03600
[InitRun 4] Iter 30, lower bound: -2.75072
[InitRun 4] Iter 40, lower bound: -2.73286
[InitRun 4] Converged at iteration 41, lower bound=-2.73266
Mean per-sample log-likelihood: -2.7324881553649902
Std per-sample log-likelihood: 1.2182121276855469
Lower bound: -2.732619047164917
Number of iterations: 38
Sum of log-likelihoods: -10929.9521484375
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Define your color map for up to 4 components (adjust if needed)
colors = [&#39;#1b9e77&#39;, &#39;#d95f02&#39;, &#39;#7570b3&#39;, &#39;#e7298a&#39;]
color_map = {label: color for label, color in enumerate(colors)}

# 1) Prepare your figure
fig, axs = plt.subplots(1, 3, figsize=(18, 6))

###############################################################################
# Plot (1) - Per-Sample Log-Likelihood
###############################################################################

scatter = axs[0].scatter(
    X[:, 0],
    X[:, 1],
    c=log_probs,
    cmap=&#39;viridis&#39;,
    s=2
)
axs[0].set_title(&#39;Per-Sample Log-Likelihood&#39;)
axs[0].set_xlabel(&#39;Feature 1&#39;)
axs[0].set_ylabel(&#39;Feature 2&#39;)
cbar = fig.colorbar(scatter, ax=axs[0])
cbar.set_label(&#39;Log-Likelihood&#39;)

###############################################################################
# Plot (2) - Predicted Cluster Labels
###############################################################################

for cluster in range(n_components):
    cluster_points = X[y_pred == cluster]
    axs[1].scatter(
        cluster_points[:, 0],
        cluster_points[:, 1],
        label=f&#39;Cluster {cluster + 1}&#39;,
        color=color_map[cluster],
        s=2
    )

axs[1].set_title(&#39;Predicted Cluster Labels&#39;)
axs[1].set_xlabel(&#39;Feature 1&#39;)
axs[1].set_ylabel(&#39;Feature 2&#39;)
axs[1].legend(title=&quot;Clusters&quot;, loc=&#39;upper right&#39;)

###############################################################################
# Plot (3) - Generated Samples from the GMM
###############################################################################

for cluster in range(n_components):
    cluster_samples = gmm_samples[gmm_labels == cluster]
    axs[2].scatter(
        cluster_samples[:, 0],
        cluster_samples[:, 1],
        label=f&#39;Cluster {cluster + 1}&#39;,
        color=color_map[cluster],
        s=2
    )

axs[2].set_title(&#39;Generated Samples from GMM&#39;)
axs[2].set_xlabel(&#39;Feature 1&#39;)
axs[2].set_ylabel(&#39;Feature 2&#39;)
axs[2].legend(title=&quot;Clusters&quot;, loc=&#39;upper right&#39;)

###############################################################################
# Final Layout &amp; Save
###############################################################################
plt.tight_layout()
plt.show()
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_gmm_6_0.png" src="../_images/notebooks_gmm_6_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Observed Data
fig, axs = plt.subplots(2, 2, figsize=(12, 10))

for k, ax in enumerate(axs.ravel()):
    prob_k = probs[:, k]
    scatter = ax.scatter(X[:, 0], X[:, 1], c=prob_k.detach().cpu().numpy(), cmap=&#39;viridis&#39;, s=2)
    ax.set_title(f&#39;Component {k+1} Probability&#39;)
    ax.set_xlabel(&#39;Feature 1&#39;)
    ax.set_ylabel(&#39;Feature 2&#39;)
    cbar = fig.colorbar(scatter, ax=ax)
    cbar.set_label(&#39;Probability&#39;)

fig.suptitle(&quot;Observed Data: Probability Distributions Across GMM Components&quot;, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

# Generated Samples
fig, axs = plt.subplots(2, 2, figsize=(12, 10))

for k, ax in enumerate(axs.ravel()):
    prob_k = generated_probs[:, k]
    scatter = ax.scatter(gmm_samples[:, 0], gmm_samples[:, 1], c=prob_k, cmap=&#39;inferno&#39;, s=2)
    ax.set_title(f&#39;Component {k+1} Probability&#39;)
    ax.set_xlabel(&#39;Feature 1&#39;)
    ax.set_ylabel(&#39;Feature 2&#39;)
    cbar = fig.colorbar(scatter, ax=ax)
    cbar.set_label(&#39;Probability&#39;)

fig.suptitle(&quot;Generated Samples: Probability Distributions Across GMM Components&quot;, fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_gmm_7_0.png" src="../_images/notebooks_gmm_7_0.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_gmm_7_1.png" src="../_images/notebooks_gmm_7_1.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>cov_types = [&#39;spherical&#39;, &#39;diag&#39;, &#39;full&#39;, &#39;tied_full&#39;, &#39;tied_diag&#39;, &#39;tied_spherical&#39;]

fig, axs = plt.subplots(2, 3, figsize=(12, 8))
axs = axs.flatten()

for ax, cov_type in zip(axs, cov_types):
    gmm = GaussianMixture(
        n_features=2,
        n_components=4,
        covariance_type=cov_type,
        init_params=&#39;kmeans&#39;,
        random_state=random_state,
        device=device
    )
    gmm.fit(X_tensor)
    labels_pred = gmm.predict(X_tensor).cpu().numpy()
    plot_gmm_results(gmm, X, labels_pred, ax=ax, title=f&#39;Covariance: {cov_type}&#39;)

plt.tight_layout()
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_gmm_8_0.png" src="../_images/notebooks_gmm_8_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>cov_types = [&#39;full&#39;, &#39;diag&#39;, &#39;spherical&#39;]
init_methods = [&#39;random&#39;, &#39;points&#39;, &#39;kpp&#39;, &#39;kmeans&#39;, &#39;maxdist&#39;]


from utils.gmm_init import GMMInitializer

def get_init_means(method, data, k):
    &quot;&quot;&quot;
    Returns initial means for a given method from GMMInitializer.
    &quot;&quot;&quot;
    if method == &#39;random&#39;:
        return GMMInitializer.random(data, k)
    elif method == &#39;points&#39;:
        return GMMInitializer.points(data, k)
    elif method == &#39;kpp&#39;:
        return GMMInitializer.kpp(data, k)
    elif method == &#39;kmeans&#39;:
        return GMMInitializer.kmeans(data, k)
    elif method == &#39;maxdist&#39;:
        return GMMInitializer.maxdist(data, k)
    else:
        raise ValueError(f&quot;Unknown init method: {method}&quot;)


import matplotlib.pyplot as plt

cov_types = [&#39;full&#39;, &#39;diag&#39;, &#39;spherical&#39;]
init_methods = [&#39;random&#39;, &#39;points&#39;, &#39;kpp&#39;, &#39;kmeans&#39;, &#39;maxdist&#39;]

fig, axs = plt.subplots(
    nrows=len(cov_types),
    ncols=len(init_methods),
    figsize=(25, 15)  # adjust as you like
)

for row_idx, cov_type in enumerate(cov_types):
    for col_idx, method in enumerate(init_methods):
        ax = axs[row_idx, col_idx]

        # 1) Get initial means from our GMMInitializer
        init_means = get_init_means(method, X_tensor, k=4)

        # 2) Create GMM with &#39;means_init&#39; so it uses these means
        gmm = GaussianMixture(
            n_features=2,
            n_components=4,
            covariance_type=cov_type,
            means_init=init_means,
            init_params=None,      # or &#39;none&#39; to bypass internal logic
            random_state=42,
            device=device
        )

        # 3) Fit the GMM on X_tensor
        gmm.fit(X_tensor)
        # 4) Predict cluster labels
        labels_pred = gmm.predict(X_tensor).cpu().numpy()

        # 5) Plot
        title = f&#39;Cov: {cov_type}, Init: {method}&#39;
        plot_gmm_results(gmm, X, labels_pred, ax=ax, title=title, init_means=init_means)

plt.tight_layout()
plt.show()
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_gmm_9_0.png" src="../_images/notebooks_gmm_9_0.png" />
</div>
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../source/tutorials.html" class="btn btn-neutral float-left" title="Tutorials" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="metrics.html" class="btn btn-neutral float-right" title="Metrics" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Adrián A. Sousa-Poza.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>